{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as A\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn.manifold \n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "from numpy  import array\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensoBoardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "#SummaryWriter encapsulates everything\n",
    "# writer = SummaryWriter('runs/exp-2')\n",
    "#creates writer object. The log will be saved in 'runs/exp-1'\n",
    "testWriter = SummaryWriter()\n",
    "#creates writer2 object with auto generated file name, the dir will be something like 'runs/Aug20-17-20-33'\n",
    "# writer3 = SummaryWriter(comment='3x learning rate')\n",
    "#creates writer3 object with auto generated file name, the comment will be appended to the filename. The dir will be something like 'runs/Aug20-17-20-33-3xlearning rate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_filename = '../data/testEmojis2.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(corpus_filename).read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlyEmojiSequences(tokens):\n",
    "    threshold_emojis = [x for x in tokens if len(x) > 1]\n",
    "    return threshold_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "emojiSequences = onlyEmojiSequences(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "# this is just the very basic translation both ways plus the length of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "\n",
    "# sole purpose of this is to have pairs! of target and context word\n",
    "# super simple once you have figured out the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "# this is a one hot encoded something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epo 0: 1.2126572132110596\n",
      "Loss at epo 1: 1.1335655450820923\n",
      "Loss at epo 2: 1.1303975582122803\n",
      "Loss at epo 3: 1.1279289722442627\n",
      "Loss at epo 4: 1.1252883672714233\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float \n",
    "dimensionSize = 300\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "firstLayer = torch.randn(dimensionSize, vocabulary_size, requires_grad=True).float()\n",
    "secondLayer = torch.randn(vocabulary_size, dimensionSize, requires_grad=True).float()\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "\n",
    "\n",
    "        # one hot encoded tensor\n",
    "        x = get_input_layer(data).float()\n",
    "        # target word \n",
    "        y_true = torch.from_numpy(np.array([target])).long()\n",
    "\n",
    "        # Hidden Layer: gradient magic happening ...\n",
    "        z1 = torch.matmul(firstLayer, x)\n",
    "        z2 = torch.matmul(secondLayer, z1)\n",
    "\n",
    "        # introducing non-linearity\n",
    "        softmax = A.LogSoftmax(dim=0)\n",
    "        soft_max_output = softmax(z2)\n",
    "\n",
    "        # neg_log_likelihood\n",
    "        loss = F.nll_loss(soft_max_output.view(1,-1), y_true)\n",
    "        loss_val += loss  # this might be to please the  framework, and adding stuff to the gradient calculator\n",
    "        \n",
    "        # propagating it back\n",
    "        loss.backward()\n",
    "        \n",
    "        # updating the weights of both layers\n",
    "        firstLayer.data -= learning_rate * firstLayer.grad.data\n",
    "        secondLayer.data -= learning_rate * secondLayer.grad.data\n",
    "\n",
    "        # set the gradients to zero for next iteration\n",
    "        firstLayer.grad.data.zero_()\n",
    "        secondLayer.grad.data.zero_()\n",
    "        \n",
    "    # this keeps track of the loss, hopefully it does converge\n",
    "#     if epo % 100 == 0:    \n",
    "#         print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')\n",
    "\n",
    "    print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')\n",
    "    testWriter.add_scalar('lossvalue', loss_val, epo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SummaryWriter.close of <tensorboardX.writer.SummaryWriter object at 0x1a1ebcf4e0>>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testWriter.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-38-7436c0911084>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-38-7436c0911084>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tensorboard runs\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tensorboard runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
