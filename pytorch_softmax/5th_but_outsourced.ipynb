{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as A\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn.manifold \n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import codecs\n",
    "from numpy  import array\n",
    "from scipy import stats\n",
    "# from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Preprocess\n",
    "prep  = Preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(prep.vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "# this is a one hot encoded something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testWriter = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_training():\n",
    "    change = priorLoss / avgLoss.item()\n",
    "    if (priorLoss == 0 or change > 1.05):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epo 0: 5.876072406768799\n",
      "Loss at epo 1: 4.792679309844971\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float \n",
    "dimensionSize = 4\n",
    "num_epochs = 2\n",
    "learning_rate = 0.001\n",
    "priorLoss = 0\n",
    "avgLoss = 0\n",
    "\n",
    "firstLayer = torch.randn(dimensionSize, prep.vocabulary_size, requires_grad=True).float()\n",
    "secondLayer = torch.randn(prep.vocabulary_size, dimensionSize, requires_grad=True).float()\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in prep.idx_pairs:\n",
    "\n",
    "        # one hot encoded tensor\n",
    "        x = get_input_layer(data).float()\n",
    "\n",
    "        # target word \n",
    "        y_true = torch.from_numpy(np.array([target])).long()\n",
    "\n",
    "        # Hidden Layer: gradient magic happening ...\n",
    "        z1 = torch.matmul(firstLayer, x)\n",
    "        z2 = torch.matmul(secondLayer, z1)\n",
    "\n",
    "        # introducing non-linearity\n",
    "        softmax = A.LogSoftmax()\n",
    "        soft_max_output = softmax(z2)\n",
    "        # neg_log_likelihood\n",
    "        loss = F.nll_loss(soft_max_output.view(1,-1), y_true)\n",
    "        loss_val += loss  # this might be to please the  framework, and adding stuff to the gradient calculator\n",
    "        \n",
    "        # propagating it back\n",
    "        loss.backward()\n",
    "        \n",
    "        # updating the weights of both layers\n",
    "        firstLayer.data -= learning_rate * firstLayer.grad.data\n",
    "        secondLayer.data -= learning_rate * secondLayer.grad.data\n",
    "\n",
    "        # set the gradients to zero for next iteration\n",
    "        firstLayer.grad.data.zero_()\n",
    "        secondLayer.grad.data.zero_()\n",
    "    avgLoss = loss_val/len(prep.idx_pairs)\n",
    "    print(f'Loss at epo {epo}: {avgLoss}')\n",
    "#     testWriter.add_scalar('lossvalue', avgLoss, epo)\n",
    "    if stop_training():\n",
    "        break\n",
    "    else:\n",
    "        priorLoss = avgLoss.item()\n",
    "\n",
    "#export scalar data to JSON for external processing\n",
    "# writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "# testWriter.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0604, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.nll_loss(soft_max_output.view(1,-1), y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"SOFTMAX_trained\"):\n",
    "    os.makedirs(\"SOFTMAX_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(secondLayer, os.path.join(\"SOFTMAX_trained\", \"test#2.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATING TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedLayer = torch.load(os.path.join(\"SOFTMAX_trained\", \"5percent#1.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts the 508 Annotator Results as the Gold-Standard\n",
    "corpus_filename = '../data/EmoSim508.json'\n",
    "corpus = open(corpus_filename).read()\n",
    "annotator_similarity_score_508 = list(array(re.findall('(?<=_Annotator_Agreement\": )(.*?)(?=\\})', corpus)))\n",
    "\n",
    "# extract Wijeratne's Cosine_Similarities of the model which was trained on Google_Sense_Labels\n",
    "google_sense_labels_score_508 = list(array(re.findall('(?<=Google_Sense_Label\": )(.*?)(?=\\,)', corpus)))\n",
    "\n",
    "# glyph_pairs_1016\n",
    "unicode_pairs_1016 = re.findall('(?<=unicodelong\": \"\\\\\\)(.*?)(?=\")', corpus)    \n",
    "glyph_pairs_1016 = [codecs.decode(unicode_pairs_1016[x].replace(str('\\\\\\\\'),str('\\\\')).replace('_',''), 'unicode_escape') for x in range(len(unicode_pairs_1016))]\n",
    "\n",
    "# computation of Cosine Similarity\n",
    "goldstandard = []\n",
    "selftrained = []\n",
    "google_sense_labels = []\n",
    "for x in range(len(annotator_similarity_score_508)):\n",
    "    cosineSimilarity = None\n",
    "    \n",
    "    emoji1 = glyph_pairs_1016.pop(0)\n",
    "    emoji2 = glyph_pairs_1016.pop(0)\n",
    "    \n",
    "    try:\n",
    "        cosineSimilarity = cosine_similarity(loadedLayer.detach().cpu().numpy()[word2idx[emoji1]].reshape(-1,300), loadedLayer.detach().cpu().numpy()[word2idx[emoji2]].reshape(-1,300))[0][0]\n",
    "    except:\n",
    "        print('the cosine similarity between ' + emoji1 + ' and ' + emoji2 + ' could not be computed.')\n",
    "    \n",
    "    if(cosineSimilarity is not None):\n",
    "        goldstandard.append(annotator_similarity_score_508.pop(0))\n",
    "        selftrained.append(cosineSimilarity)\n",
    "        google_sense_labels.append(float(google_sense_labels_score_508.pop(0)))\n",
    "        \n",
    "\n",
    "# skalierter GoldStandard\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "scaled_goldstandard = min_max_scaler.fit_transform(np.asarray(goldstandard).reshape(-1, 1))\n",
    "\n",
    "print()\n",
    "\n",
    "# computation of SPEARRANK CORRELATION COEFFICIENT\n",
    "meinSPEARMAN = stats.spearmanr(goldstandard, selftrained)\n",
    "seinSPEARMAN = stats.spearmanr(goldstandard, google_sense_labels)\n",
    "print('mein Spearman: {}'.format(meinSPEARMAN.correlation))\n",
    "print('sein Spearman: {}'.format(seinSPEARMAN.correlation))\n",
    "\n",
    "\n",
    "# computation of MAE\n",
    "meinMAE = mean_absolute_error(scaled_goldstandard, min_max_scaler.fit_transform(np.asarray(selftrained).reshape(-1, 1)))\n",
    "seinMAE = mean_absolute_error(scaled_goldstandard, google_sense_labels)\n",
    "print('mein MAE ist {}'.format(meinMAE))\n",
    "print('sein MAE ist {}'.format(seinMAE))\n",
    "\n",
    "\n",
    "# computation of MSE\n",
    "meinMSE = mean_squared_error(scaled_goldstandard, min_max_scaler.fit_transform(np.asarray(selftrained).reshape(-1, 1)))\n",
    "seinMSE = mean_squared_error(scaled_goldstandard, google_sense_labels)\n",
    "print('mein MSE ist {}'.format(meinMSE))\n",
    "print('sein MSE ist {}'.format(seinMSE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    ">>> # each element in target has to have 0 <= value < C\n",
    ">>> target = torch.tensor([1, 0, 4])\n",
    ">>> output = F.nll_loss(F.log_softmax(input), target)\n",
    ">>> output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.randn(1200, 5)\n",
    "tensor2 = torch.randn(5, 80)\n",
    "torch.matmul(tensor1, tensor2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.randn(2, 2)\n",
    "print(tensor1)\n",
    "print(torch.t(tensor1))\n",
    "# tensor1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
