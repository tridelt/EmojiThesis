{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as A\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn.manifold \n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import codecs\n",
    "from numpy  import array\n",
    "from scipy import stats\n",
    "# from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_filename = '../data/extracted_emoji_sequences.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(corpus_filename).read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlyEmojiSequences(tokens):\n",
    "    threshold_emojis = [x for x in tokens if len(x) > 1]\n",
    "    return threshold_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "emojiSequences = onlyEmojiSequences(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "# this is just the very basic translation both ways plus the length of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 8\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "\n",
    "# sole purpose of this is to have pairs! of target and context word\n",
    "# super simple once you have figured out the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "# this is a one hot encoded something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testWriter = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_training():\n",
    "    change = priorLoss / avgLoss.item()\n",
    "    if (priorLoss == 0 or change > 1.05):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epo 0: 15.886506080627441\n",
      "Loss at epo 1: 13.477213859558105\n",
      "Loss at epo 2: 11.905130386352539\n",
      "Loss at epo 3: 10.845651626586914\n",
      "Loss at epo 4: 9.965472221374512\n",
      "Loss at epo 5: 9.1500244140625\n",
      "Loss at epo 6: 8.50841236114502\n",
      "Loss at epo 7: 8.1043119430542\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float \n",
    "dimensionSize = 300\n",
    "num_epochs = 80\n",
    "learning_rate = 0.001\n",
    "priorLoss = 0\n",
    "avgLoss = 0\n",
    "\n",
    "firstLayer = torch.randn(dimensionSize, vocabulary_size, requires_grad=True).float()\n",
    "secondLayer = torch.randn(vocabulary_size, dimensionSize, requires_grad=True).float()\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "\n",
    "        # one hot encoded tensor\n",
    "        x = get_input_layer(data).float()\n",
    "        # target word \n",
    "        y_true = torch.from_numpy(np.array([target])).long()\n",
    "\n",
    "        # Hidden Layer: gradient magic happening ...\n",
    "        z1 = torch.matmul(firstLayer, x)\n",
    "        z2 = torch.matmul(secondLayer, z1)\n",
    "\n",
    "        # introducing non-linearity\n",
    "        softmax = A.LogSoftmax(dim=0)\n",
    "        soft_max_output = softmax(z2)\n",
    "\n",
    "        # neg_log_likelihood\n",
    "        loss = F.nll_loss(soft_max_output.view(1,-1), y_true)\n",
    "        loss_val += loss  # this might be to please the  framework, and adding stuff to the gradient calculator\n",
    "        \n",
    "        # propagating it back\n",
    "        loss.backward()\n",
    "        \n",
    "        # updating the weights of both layers\n",
    "        firstLayer.data -= learning_rate * firstLayer.grad.data\n",
    "        secondLayer.data -= learning_rate * secondLayer.grad.data\n",
    "\n",
    "        # set the gradients to zero for next iteration\n",
    "        firstLayer.grad.data.zero_()\n",
    "        secondLayer.grad.data.zero_()\n",
    "    avgLoss = loss_val/len(idx_pairs)\n",
    "    print(f'Loss at epo {epo}: {avgLoss}')\n",
    "#     testWriter.add_scalar('lossvalue', avgLoss, epo)\n",
    "    if stop_training():\n",
    "        break\n",
    "    else:\n",
    "        priorLoss = avgLoss.item()\n",
    "\n",
    "# export scalar data to JSON for external processing\n",
    "# writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "# testWriter.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"SOFTMAX_trained\"):\n",
    "    os.makedirs(\"SOFTMAX_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(secondLayer, os.path.join(\"SOFTMAX_trained\", \"test.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATING TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedLayer = torch.load(os.path.join(\"SOFTMAX_trained\", \"5percent#1.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cosine similarity between ğŸ‡¬ğŸ‡§ and ğŸ‡ºğŸ‡¸ could not be computed.\n",
      "the cosine similarity between ğŸ… and ğŸ‡ºğŸ‡¸ could not be computed.\n",
      "the cosine similarity between ğŸ‡ºğŸ‡¸ and â¤ could not be computed.\n",
      "the cosine similarity between ğŸ‡ºğŸ‡¸ and ğŸ’¥ could not be computed.\n",
      "the cosine similarity between ğŸ¤ and ğŸ‡³ğŸ‡¬ could not be computed.\n",
      "the cosine similarity between ğŸ‡³ğŸ‡¬ and ğŸ“² could not be computed.\n",
      "the cosine similarity between ğŸ‘‡ and ğŸ‡³ğŸ‡¬ could not be computed.\n",
      "the cosine similarity between ğŸ§ and ğŸ‡³ğŸ‡¬ could not be computed.\n",
      "the cosine similarity between ğŸ‡³ğŸ‡¬ and ğŸ¶ could not be computed.\n",
      "\n",
      "mein Spearman: 0.26081293646823833\n",
      "sein Spearman: 0.7592834163731204\n",
      "mein MAE ist 0.2823634684085846\n",
      "sein MAE ist 0.23933867735470943\n",
      "mein MSE ist 0.1086303488426247\n",
      "sein MSE ist 0.07912399799599198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype <U4 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# extracts the 508 Annotator Results as the Gold-Standard\n",
    "corpus_filename = '../data/EmoSim508.json'\n",
    "corpus = open(corpus_filename).read()\n",
    "annotator_similarity_score_508 = list(array(re.findall('(?<=_Annotator_Agreement\": )(.*?)(?=\\})', corpus)))\n",
    "\n",
    "# extract Wijeratne's Cosine_Similarities of the model which was trained on Google_Sense_Labels\n",
    "google_sense_labels_score_508 = list(array(re.findall('(?<=Google_Sense_Label\": )(.*?)(?=\\,)', corpus)))\n",
    "\n",
    "# glyph_pairs_1016\n",
    "unicode_pairs_1016 = re.findall('(?<=unicodelong\": \"\\\\\\)(.*?)(?=\")', corpus)    \n",
    "glyph_pairs_1016 = [codecs.decode(unicode_pairs_1016[x].replace(str('\\\\\\\\'),str('\\\\')).replace('_',''), 'unicode_escape') for x in range(len(unicode_pairs_1016))]\n",
    "\n",
    "# computation of Cosine Similarity\n",
    "goldstandard = []\n",
    "selftrained = []\n",
    "google_sense_labels = []\n",
    "for x in range(len(annotator_similarity_score_508)):\n",
    "    cosineSimilarity = None\n",
    "    \n",
    "    emoji1 = glyph_pairs_1016.pop(0)\n",
    "    emoji2 = glyph_pairs_1016.pop(0)\n",
    "    \n",
    "    try:\n",
    "        cosineSimilarity = cosine_similarity(loadedLayer.detach().cpu().numpy()[word2idx[emoji1]].reshape(-1,300), loadedLayer.detach().cpu().numpy()[word2idx[emoji2]].reshape(-1,300))[0][0]\n",
    "    except:\n",
    "        print('the cosine similarity between ' + emoji1 + ' and ' + emoji2 + ' could not be computed.')\n",
    "    \n",
    "    if(cosineSimilarity is not None):\n",
    "        goldstandard.append(annotator_similarity_score_508.pop(0))\n",
    "        selftrained.append(cosineSimilarity)\n",
    "        google_sense_labels.append(float(google_sense_labels_score_508.pop(0)))\n",
    "        \n",
    "\n",
    "# skalierter GoldStandard\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "scaled_goldstandard = min_max_scaler.fit_transform(np.asarray(goldstandard).reshape(-1, 1))\n",
    "\n",
    "print()\n",
    "\n",
    "# computation of SPEARRANK CORRELATION COEFFICIENT\n",
    "meinSPEARMAN = stats.spearmanr(goldstandard, selftrained)\n",
    "seinSPEARMAN = stats.spearmanr(goldstandard, google_sense_labels)\n",
    "print('mein Spearman: {}'.format(meinSPEARMAN.correlation))\n",
    "print('sein Spearman: {}'.format(seinSPEARMAN.correlation))\n",
    "\n",
    "\n",
    "# computation of MAE\n",
    "meinMAE = mean_absolute_error(scaled_goldstandard, min_max_scaler.fit_transform(np.asarray(selftrained).reshape(-1, 1)))\n",
    "seinMAE = mean_absolute_error(scaled_goldstandard, google_sense_labels)\n",
    "print('mein MAE ist {}'.format(meinMAE))\n",
    "print('sein MAE ist {}'.format(seinMAE))\n",
    "\n",
    "\n",
    "# computation of MSE\n",
    "meinMSE = mean_squared_error(scaled_goldstandard, min_max_scaler.fit_transform(np.asarray(selftrained).reshape(-1, 1)))\n",
    "seinMSE = mean_squared_error(scaled_goldstandard, google_sense_labels)\n",
    "print('mein MSE ist {}'.format(meinMSE))\n",
    "print('sein MSE ist {}'.format(seinMSE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41808996],\n",
       "       [0.55868196],\n",
       "       [0.62463534],\n",
       "       [0.613366  ],\n",
       "       [0.91815275],\n",
       "       [0.43810028],\n",
       "       [0.35566697],\n",
       "       [0.7824482 ],\n",
       "       [0.48623937],\n",
       "       [0.4098994 ],\n",
       "       [0.5206271 ],\n",
       "       [0.58374447],\n",
       "       [0.40557975],\n",
       "       [0.77905566],\n",
       "       [0.30938104],\n",
       "       [0.82548654],\n",
       "       [0.2149996 ],\n",
       "       [0.6475072 ],\n",
       "       [0.3284202 ],\n",
       "       [0.58717144],\n",
       "       [0.41859832],\n",
       "       [0.62164295],\n",
       "       [0.25507906],\n",
       "       [0.46167788],\n",
       "       [0.5737109 ],\n",
       "       [0.5919618 ],\n",
       "       [0.18610746],\n",
       "       [0.79737103],\n",
       "       [0.2863143 ],\n",
       "       [0.7752216 ],\n",
       "       [0.2578386 ],\n",
       "       [1.        ],\n",
       "       [0.40357107],\n",
       "       [0.42324013],\n",
       "       [0.37912372],\n",
       "       [0.73510385],\n",
       "       [0.3616745 ],\n",
       "       [0.7467352 ],\n",
       "       [0.6437603 ],\n",
       "       [0.4741466 ],\n",
       "       [0.60268736],\n",
       "       [0.59661746],\n",
       "       [0.5867232 ],\n",
       "       [0.420199  ],\n",
       "       [0.6808141 ],\n",
       "       [0.5077671 ],\n",
       "       [0.50645936],\n",
       "       [0.51853704],\n",
       "       [0.28801358],\n",
       "       [0.53284657],\n",
       "       [0.6925435 ],\n",
       "       [0.7659864 ],\n",
       "       [0.43768448],\n",
       "       [0.38866264],\n",
       "       [0.40452597],\n",
       "       [0.4861436 ],\n",
       "       [0.3795319 ],\n",
       "       [0.5597528 ],\n",
       "       [0.54631734],\n",
       "       [0.21581793],\n",
       "       [0.5480339 ],\n",
       "       [0.5439992 ],\n",
       "       [0.41648817],\n",
       "       [0.8116227 ],\n",
       "       [0.46365258],\n",
       "       [0.53936756],\n",
       "       [0.4123088 ],\n",
       "       [0.67228484],\n",
       "       [0.21340689],\n",
       "       [0.53708553],\n",
       "       [0.45560282],\n",
       "       [0.4879751 ],\n",
       "       [0.3572439 ],\n",
       "       [0.45686275],\n",
       "       [0.3960529 ],\n",
       "       [0.5721866 ],\n",
       "       [0.29464665],\n",
       "       [0.6320164 ],\n",
       "       [0.32850522],\n",
       "       [0.57924354],\n",
       "       [0.35783905],\n",
       "       [0.7778968 ],\n",
       "       [0.34169057],\n",
       "       [0.18282835],\n",
       "       [0.43804437],\n",
       "       [0.58495593],\n",
       "       [0.3627286 ],\n",
       "       [0.50949323],\n",
       "       [0.365125  ],\n",
       "       [0.4309428 ],\n",
       "       [0.31658304],\n",
       "       [0.38595602],\n",
       "       [0.62815946],\n",
       "       [0.27175498],\n",
       "       [0.6447573 ],\n",
       "       [0.5333608 ],\n",
       "       [0.24610484],\n",
       "       [0.27885303],\n",
       "       [0.48863566],\n",
       "       [0.36165762],\n",
       "       [0.48234558],\n",
       "       [0.30166885],\n",
       "       [0.23966122],\n",
       "       [0.49731612],\n",
       "       [0.45390362],\n",
       "       [0.63232744],\n",
       "       [0.4764647 ],\n",
       "       [0.47774398],\n",
       "       [0.2883512 ],\n",
       "       [0.53700995],\n",
       "       [0.5404041 ],\n",
       "       [0.39765123],\n",
       "       [0.37304792],\n",
       "       [0.26882896],\n",
       "       [0.41465816],\n",
       "       [0.514421  ],\n",
       "       [0.5812455 ],\n",
       "       [0.4296006 ],\n",
       "       [0.36283267],\n",
       "       [0.4903722 ],\n",
       "       [0.3679389 ],\n",
       "       [0.19719017],\n",
       "       [0.18459551],\n",
       "       [0.26989716],\n",
       "       [0.53965664],\n",
       "       [0.18626852],\n",
       "       [0.46302733],\n",
       "       [0.55698746],\n",
       "       [0.3715871 ],\n",
       "       [0.51320094],\n",
       "       [0.49227175],\n",
       "       [0.5205089 ],\n",
       "       [0.45628172],\n",
       "       [0.44293684],\n",
       "       [0.32736105],\n",
       "       [0.40676516],\n",
       "       [0.2265424 ],\n",
       "       [0.6140085 ],\n",
       "       [0.7274854 ],\n",
       "       [0.5651706 ],\n",
       "       [0.62634337],\n",
       "       [0.48477387],\n",
       "       [0.3753652 ],\n",
       "       [0.40467358],\n",
       "       [0.46480072],\n",
       "       [0.2300358 ],\n",
       "       [0.5237751 ],\n",
       "       [0.72097516],\n",
       "       [0.61601675],\n",
       "       [0.42506623],\n",
       "       [0.43037176],\n",
       "       [0.29413906],\n",
       "       [0.62773854],\n",
       "       [0.22299626],\n",
       "       [0.44610694],\n",
       "       [0.57852495],\n",
       "       [0.42504603],\n",
       "       [0.28886235],\n",
       "       [0.30322197],\n",
       "       [0.47791767],\n",
       "       [0.21077363],\n",
       "       [0.38628137],\n",
       "       [0.51085216],\n",
       "       [0.27775997],\n",
       "       [0.25997996],\n",
       "       [0.56627053],\n",
       "       [0.43800285],\n",
       "       [0.39510822],\n",
       "       [0.6146537 ],\n",
       "       [0.58373165],\n",
       "       [0.4966896 ],\n",
       "       [0.4444905 ],\n",
       "       [0.6198719 ],\n",
       "       [0.3821056 ],\n",
       "       [0.52773494],\n",
       "       [0.46025825],\n",
       "       [0.57830024],\n",
       "       [0.52533877],\n",
       "       [0.61148965],\n",
       "       [0.42685544],\n",
       "       [0.60064447],\n",
       "       [0.17714451],\n",
       "       [0.5097689 ],\n",
       "       [0.397234  ],\n",
       "       [0.5556011 ],\n",
       "       [0.38539875],\n",
       "       [0.31961522],\n",
       "       [0.4083487 ],\n",
       "       [0.50690734],\n",
       "       [0.7782268 ],\n",
       "       [0.41728184],\n",
       "       [0.2551463 ],\n",
       "       [0.49247313],\n",
       "       [0.15670681],\n",
       "       [0.28881118],\n",
       "       [0.15519792],\n",
       "       [0.32963264],\n",
       "       [0.5562343 ],\n",
       "       [0.23237626],\n",
       "       [0.2784038 ],\n",
       "       [0.56894815],\n",
       "       [0.4199163 ],\n",
       "       [0.62173045],\n",
       "       [0.41473308],\n",
       "       [0.2969317 ],\n",
       "       [0.55860174],\n",
       "       [0.5057705 ],\n",
       "       [0.38871837],\n",
       "       [0.85474   ],\n",
       "       [0.15377262],\n",
       "       [0.2945295 ],\n",
       "       [0.34818324],\n",
       "       [0.4107648 ],\n",
       "       [0.63726944],\n",
       "       [0.5661467 ],\n",
       "       [0.69866675],\n",
       "       [0.5712189 ],\n",
       "       [0.48792058],\n",
       "       [0.46563095],\n",
       "       [0.7237517 ],\n",
       "       [0.31854105],\n",
       "       [0.4891805 ],\n",
       "       [0.19312905],\n",
       "       [0.48391128],\n",
       "       [0.28836164],\n",
       "       [0.19256803],\n",
       "       [0.43307716],\n",
       "       [0.5635395 ],\n",
       "       [0.5826763 ],\n",
       "       [0.460617  ],\n",
       "       [0.2917488 ],\n",
       "       [0.26428065],\n",
       "       [0.43737745],\n",
       "       [0.52446455],\n",
       "       [0.25616583],\n",
       "       [0.43704754],\n",
       "       [0.6487724 ],\n",
       "       [0.53482413],\n",
       "       [0.23496398],\n",
       "       [0.5052742 ],\n",
       "       [0.47772807],\n",
       "       [0.6405416 ],\n",
       "       [0.77346814],\n",
       "       [0.2594616 ],\n",
       "       [0.33911625],\n",
       "       [0.44393677],\n",
       "       [0.5004664 ],\n",
       "       [0.66859055],\n",
       "       [0.46789384],\n",
       "       [0.72379005],\n",
       "       [0.35205528],\n",
       "       [0.2757628 ],\n",
       "       [0.6710492 ],\n",
       "       [0.5021726 ],\n",
       "       [0.49057695],\n",
       "       [0.67703885],\n",
       "       [0.6014489 ],\n",
       "       [0.5417726 ],\n",
       "       [0.3890044 ],\n",
       "       [0.4600073 ],\n",
       "       [0.4248883 ],\n",
       "       [0.31213018],\n",
       "       [0.5064771 ],\n",
       "       [0.25418884],\n",
       "       [0.6676543 ],\n",
       "       [0.31070942],\n",
       "       [0.30341542],\n",
       "       [0.56926095],\n",
       "       [0.33619547],\n",
       "       [0.68576396],\n",
       "       [0.30513147],\n",
       "       [0.37926075],\n",
       "       [0.39512268],\n",
       "       [0.54210305],\n",
       "       [0.3397859 ],\n",
       "       [0.49337995],\n",
       "       [0.4975142 ],\n",
       "       [0.44730425],\n",
       "       [0.29529023],\n",
       "       [0.5446116 ],\n",
       "       [0.36180538],\n",
       "       [0.8512518 ],\n",
       "       [0.40543723],\n",
       "       [0.6756825 ],\n",
       "       [0.3539251 ],\n",
       "       [0.26425737],\n",
       "       [0.6182978 ],\n",
       "       [0.649086  ],\n",
       "       [0.61419785],\n",
       "       [0.362033  ],\n",
       "       [0.48258448],\n",
       "       [0.7951579 ],\n",
       "       [0.2671523 ],\n",
       "       [0.3218683 ],\n",
       "       [0.5652016 ],\n",
       "       [0.71297604],\n",
       "       [0.38751745],\n",
       "       [0.7196382 ],\n",
       "       [0.38892466],\n",
       "       [0.53513324],\n",
       "       [0.4483543 ],\n",
       "       [0.21339093],\n",
       "       [0.4710436 ],\n",
       "       [0.30976045],\n",
       "       [0.60652256],\n",
       "       [0.31402624],\n",
       "       [0.08161445],\n",
       "       [0.27583846],\n",
       "       [0.34293067],\n",
       "       [0.3462088 ],\n",
       "       [0.4578266 ],\n",
       "       [0.5363936 ],\n",
       "       [0.70739263],\n",
       "       [0.7923835 ],\n",
       "       [0.33709264],\n",
       "       [0.3797587 ],\n",
       "       [0.35876217],\n",
       "       [0.49673223],\n",
       "       [0.38694966],\n",
       "       [0.3991541 ],\n",
       "       [0.42044556],\n",
       "       [0.25800952],\n",
       "       [0.7150835 ],\n",
       "       [0.5939722 ],\n",
       "       [0.31506175],\n",
       "       [0.24487624],\n",
       "       [0.2244794 ],\n",
       "       [0.4114019 ],\n",
       "       [0.4274763 ],\n",
       "       [0.31268704],\n",
       "       [0.36509624],\n",
       "       [0.6142425 ],\n",
       "       [0.4254505 ],\n",
       "       [0.359378  ],\n",
       "       [0.5389741 ],\n",
       "       [0.49092975],\n",
       "       [0.47347066],\n",
       "       [0.33551717],\n",
       "       [0.4664167 ],\n",
       "       [0.6348094 ],\n",
       "       [0.633757  ],\n",
       "       [0.49770468],\n",
       "       [0.3671388 ],\n",
       "       [0.4718732 ],\n",
       "       [0.23653671],\n",
       "       [0.68883634],\n",
       "       [0.44986778],\n",
       "       [0.61731064],\n",
       "       [0.51817876],\n",
       "       [0.6830549 ],\n",
       "       [0.17149341],\n",
       "       [0.40452135],\n",
       "       [0.4211191 ],\n",
       "       [0.24551043],\n",
       "       [0.25012502],\n",
       "       [0.25400063],\n",
       "       [0.74794674],\n",
       "       [0.7109108 ],\n",
       "       [0.        ],\n",
       "       [0.6371286 ],\n",
       "       [0.6079556 ],\n",
       "       [0.53955674],\n",
       "       [0.24620162],\n",
       "       [0.42593157],\n",
       "       [0.28225738],\n",
       "       [0.49802548],\n",
       "       [0.5602149 ],\n",
       "       [0.4354369 ],\n",
       "       [0.3250972 ],\n",
       "       [0.31537715],\n",
       "       [0.14698927],\n",
       "       [0.49977228],\n",
       "       [0.31230566],\n",
       "       [0.3173325 ],\n",
       "       [0.58103764],\n",
       "       [0.678774  ],\n",
       "       [0.47662747],\n",
       "       [0.4130507 ],\n",
       "       [0.54544216],\n",
       "       [0.5009566 ],\n",
       "       [0.02505386],\n",
       "       [0.57498693],\n",
       "       [0.3091924 ],\n",
       "       [0.3787282 ],\n",
       "       [0.16804886],\n",
       "       [0.33276594],\n",
       "       [0.36211053],\n",
       "       [0.28035513],\n",
       "       [0.26764292],\n",
       "       [0.38933617],\n",
       "       [0.5187757 ],\n",
       "       [0.313298  ],\n",
       "       [0.4346658 ],\n",
       "       [0.61385965],\n",
       "       [0.59116745],\n",
       "       [0.46185613],\n",
       "       [0.3841483 ],\n",
       "       [0.21614136],\n",
       "       [0.28083074],\n",
       "       [0.0765468 ],\n",
       "       [0.22995079],\n",
       "       [0.6067114 ],\n",
       "       [0.31104842],\n",
       "       [0.58659106],\n",
       "       [0.3543214 ],\n",
       "       [0.15980926],\n",
       "       [0.18758085],\n",
       "       [0.45511526],\n",
       "       [0.47528267],\n",
       "       [0.7486124 ],\n",
       "       [0.20428033],\n",
       "       [0.4480061 ],\n",
       "       [0.49503773],\n",
       "       [0.31729355],\n",
       "       [0.40304837],\n",
       "       [0.49482793],\n",
       "       [0.30816555],\n",
       "       [0.13739435],\n",
       "       [0.1276193 ],\n",
       "       [0.5682155 ],\n",
       "       [0.4572385 ],\n",
       "       [0.27062812],\n",
       "       [0.08919472],\n",
       "       [0.7412741 ],\n",
       "       [0.29466096],\n",
       "       [0.25277722],\n",
       "       [0.4791606 ],\n",
       "       [0.2057826 ],\n",
       "       [0.3167356 ],\n",
       "       [0.41806912],\n",
       "       [0.43280354],\n",
       "       [0.55354095],\n",
       "       [0.5304543 ],\n",
       "       [0.39751863],\n",
       "       [0.69869065],\n",
       "       [0.40846342],\n",
       "       [0.283891  ],\n",
       "       [0.17648527],\n",
       "       [0.4444434 ],\n",
       "       [0.5026879 ],\n",
       "       [0.5866902 ],\n",
       "       [0.2409289 ],\n",
       "       [0.30413216],\n",
       "       [0.13987197],\n",
       "       [0.5543806 ],\n",
       "       [0.233377  ],\n",
       "       [0.45717323],\n",
       "       [0.34624913],\n",
       "       [0.2888138 ],\n",
       "       [0.32794535],\n",
       "       [0.16099176],\n",
       "       [0.53327143],\n",
       "       [0.38598904],\n",
       "       [0.5080321 ],\n",
       "       [0.08987816],\n",
       "       [0.25368485],\n",
       "       [0.3101211 ],\n",
       "       [0.18188657],\n",
       "       [0.31204492],\n",
       "       [0.12182444],\n",
       "       [0.3445734 ],\n",
       "       [0.32960895],\n",
       "       [0.4059775 ],\n",
       "       [0.49746504],\n",
       "       [0.14596139],\n",
       "       [0.3246226 ],\n",
       "       [0.47010088],\n",
       "       [0.70526457],\n",
       "       [0.26729444],\n",
       "       [0.6102607 ],\n",
       "       [0.06542709],\n",
       "       [0.79316616],\n",
       "       [0.08638297],\n",
       "       [0.10865398],\n",
       "       [0.4929188 ],\n",
       "       [0.12145069],\n",
       "       [0.18615942],\n",
       "       [0.36790165],\n",
       "       [0.5051736 ],\n",
       "       [0.3323548 ],\n",
       "       [0.5736029 ],\n",
       "       [0.35286608],\n",
       "       [0.35137752],\n",
       "       [0.1839477 ],\n",
       "       [0.19424593],\n",
       "       [0.02926333],\n",
       "       [0.30901882],\n",
       "       [0.6520536 ],\n",
       "       [0.220465  ],\n",
       "       [0.50462395],\n",
       "       [0.12415606],\n",
       "       [0.25418916],\n",
       "       [0.34748328],\n",
       "       [0.3080208 ],\n",
       "       [0.08175054],\n",
       "       [0.1030416 ],\n",
       "       [0.13613646],\n",
       "       [0.3166899 ],\n",
       "       [0.15986562]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler.fit_transform(np.asarray(selftrained).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
