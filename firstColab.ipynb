{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "firstColab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tridelt/EmojiThesis/blob/master/firstColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "aRgBdhJIMR5m",
        "colab_type": "code",
        "outputId": "c464d143-a8b3-4a3a-f1a5-e6731f0cf4ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 110851 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gcCJSlrYUAOk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-B9OqNzUBh3",
        "colab_type": "code",
        "outputId": "569861cd-19c1-47d5-eb65-0e3e96397316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HP3f8rHAUKom",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FNdXp6q2UMwf",
        "colab_type": "code",
        "outputId": "2b25a881-8925-4013-dcbf-a265095ecccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x5af9c000 @  0x7f9583a342a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mEozriRBxeIf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# pytorch codes starts here"
      ]
    },
    {
      "metadata": {
        "id": "Cmn1GDw-XS30",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as A\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import sklearn.manifold \n",
        "from matplotlib import pyplot as plt\n",
        "import re\n",
        "import codecs\n",
        "from numpy  import array\n",
        "from scipy import stats\n",
        "# from scipy.spatial import distance\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yPjSBe0_VtT0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corpus_filename = 'drive/data/extracted_emoji_sequences.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WMywIwC_WQ1m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corpus = open(corpus_filename).read().splitlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tM16XfY5XpL4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize_corpus(corpus):\n",
        "    tokens = [x.split() for x in corpus]\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zOB0THqPXp4t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def onlyEmojiSequences(tokens):\n",
        "    threshold_emojis = [x for x in tokens if len(x) > 1]\n",
        "    return threshold_emojis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wD3xyvhWXweD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenized_corpus = tokenize_corpus(corpus)\n",
        "emojiSequences = onlyEmojiSequences(tokenized_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CpANBMqVXx3h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocabulary = []\n",
        "for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "\n",
        "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "\n",
        "# this is just the very basic translation both ways plus the length of vocabulary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Pt8Z1oMXzfQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "window_size = 8\n",
        "idx_pairs = []\n",
        "# for each sentence\n",
        "for sentence in tokenized_corpus:\n",
        "    indices = [word2idx[word] for word in sentence]\n",
        "    # for each word, threated as center word\n",
        "    for center_word_pos in range(len(indices)):\n",
        "        # for each window position\n",
        "        for w in range(-window_size, window_size + 1):\n",
        "            context_word_pos = center_word_pos + w\n",
        "            # make soure not jump out sentence\n",
        "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
        "                continue\n",
        "            context_word_idx = indices[context_word_pos]\n",
        "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
        "\n",
        "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
        "\n",
        "# sole purpose of this is to have pairs! of target and context word\n",
        "# super simple once you have figured out the code!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m-x3yE76X07t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_input_layer(word_idx):\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    x = torch.zeros(vocabulary_size, device=device).float()\n",
        "    x[word_idx] = 1.0\n",
        "    return x\n",
        "\n",
        "# this is a one hot encoded something"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jDOFJQNTX2mE",
        "colab_type": "code",
        "outputId": "3a2fe17b-c492-4574-fb68-13426acf902e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "dtype = torch.float \n",
        "device = torch.device(\"cuda\")\n",
        "dimensionSize = 300\n",
        "num_epochs = 1\n",
        "learning_rate = 0.001\n",
        "\n",
        "firstLayer = torch.randn(dimensionSize, vocabulary_size, device=device, requires_grad=True).float()\n",
        "secondLayer = torch.randn(vocabulary_size, dimensionSize, device=device, requires_grad=True).float()\n",
        "\n",
        "for epo in range(num_epochs):\n",
        "    loss_val = 0\n",
        "    for data, target in idx_pairs:\n",
        "\n",
        "        # one hot encoded tensor\n",
        "        x = get_input_layer(data)\n",
        "        \n",
        "        # target word \n",
        "        y_true = torch.zeros(1, device=\"cuda\", requires_grad=True).long()\n",
        "        y_true[0] = torch.from_numpy(np.array([target])).long()\n",
        "#         y_true = torch.randn(1, device=device, requires_grad=True).long()\n",
        "#         y_true = torch.from_numpy(np.array([target])).long()\n",
        "\n",
        "        # Hidden Layer: gradient magic happening ...\n",
        "        z1 = torch.matmul(firstLayer, x)\n",
        "        z2 = torch.matmul(secondLayer, z1)\n",
        "\n",
        "        # introducing non-linearity\n",
        "        softmax = A.LogSoftmax(dim=0)\n",
        "        soft_max_output = softmax(z2)\n",
        "\n",
        "        # neg_log_likelihood\n",
        "        loss = F.nll_loss(soft_max_output.view(1,-1), y_true)\n",
        "        loss_val += loss  # this might be to please the  framework, and adding stuff to the gradient calculator\n",
        "        \n",
        "        # propagating it back\n",
        "        loss.backward()\n",
        "        \n",
        "        # updating the weights of both layers\n",
        "        firstLayer.data -= learning_rate * firstLayer.grad.data\n",
        "        secondLayer.data -= learning_rate * secondLayer.grad.data\n",
        "\n",
        "        # set the gradients to zero for next iteration\n",
        "        firstLayer.grad.data.zero_()\n",
        "        secondLayer.grad.data.zero_()\n",
        "        \n",
        "    # this keeps track of the loss, hopefully it does converge\n",
        "#     if epo % 1000 == 0:    \n",
        "#         print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')\n",
        "\n",
        "    print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')\n",
        "#     testWriter.add_scalar('lossvalue', loss_val, epo)\n",
        "# testWriter.close"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at epo 0: 12.102314949035645\n",
            "CPU times: user 16min 32s, sys: 3min 34s, total: 20min 6s\n",
            "Wall time: 19min 59s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M8OkacIHDEjX",
        "colab_type": "code",
        "outputId": "025319ee-3f2a-4b91-ee1b-93f3ffa2ea02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "cell_type": "code",
      "source": [
        "# extracts the 508 Annotator Results as the Gold-Standard\n",
        "corpus_filename = 'drive/data/EmoSim508.json'\n",
        "corpus = open(corpus_filename).read()\n",
        "annotator_similarity_score_508 = list(array(re.findall('(?<=_Annotator_Agreement\": )(.*?)(?=\\})', corpus)))\n",
        "\n",
        "# glyph_pairs_1016\n",
        "unicode_pairs_1016 = re.findall('(?<=unicodelong\": \"\\\\\\)(.*?)(?=\")', corpus)    \n",
        "glyph_pairs_1016 = [codecs.decode(unicode_pairs_1016[x].replace(str('\\\\\\\\'),str('\\\\')).replace('_',''), 'unicode_escape') for x in range(len(unicode_pairs_1016))]\n",
        "\n",
        "# computation of Cosine Similarity\n",
        "goldstandard = []\n",
        "selftrained = []\n",
        "for x in range(len(annotator_similarity_score_508)):\n",
        "    cosineSimilarity = None\n",
        "    \n",
        "    emoji1 = glyph_pairs_1016.pop(0)\n",
        "    emoji2 = glyph_pairs_1016.pop(0)\n",
        "    \n",
        "    try:\n",
        "        cosineSimilarity = cosine_similarity(secondLayer.detach().cpu().numpy()[word2idx[emoji1]].reshape(-1,300), secondLayer.detach().cpu().numpy()[word2idx[emoji2]].reshape(-1,300))[0][0]\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "#         print('the cosine similarity between ' + emoji1 + ' and ' + emoji2 + ' could not be computed.')\n",
        "    \n",
        "    \n",
        "    if(cosineSimilarity is not None):\n",
        "        selftrained.append(cosineSimilarity)\n",
        "        goldstandard.append(annotator_similarity_score_508.pop(0))\n",
        "\n",
        "# computation of SPEARRANK CORRELATION COEFFICIENT\n",
        "spearmanRank = stats.spearmanr(goldstandard, selftrained)\n",
        "\n",
        "print('Der Spearman Rank Correlation Coefficient ist {}'.format(spearmanRank))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'🇬🇧'\n",
            "'🇺🇸'\n",
            "'🇺🇸'\n",
            "'🇺🇸'\n",
            "'🇳🇬'\n",
            "'🇳🇬'\n",
            "'🇳🇬'\n",
            "'🇳🇬'\n",
            "'🇳🇬'\n",
            "Der Spearman Rank Correlation Coefficient ist SpearmanrResult(correlation=0.217929752879138, pvalue=8.870450943165614e-07)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
            "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "49eAzgPB0X5h",
        "colab_type": "code",
        "outputId": "34594812-33b5-4b79-99e0-24c76b7aa134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "goldstandard"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "doXfLDMb72_-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}