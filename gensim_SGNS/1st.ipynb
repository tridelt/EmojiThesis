{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy  import array\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['seed']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_filename = '../data/extracted_emoji_sequences.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(corpus_filename).read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlyEmojiSequences(tokens):\n",
    "    threshold_emojis = [x for x in tokens if len(x) > 1]\n",
    "    return threshold_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "emojiSequences = onlyEmojiSequences(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 610,256 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in emojiSequences])\n",
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality of the resulting word vectors.\n",
    "num_features = 300\n",
    "\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 2\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "# remove later\n",
    "seed = 1\n",
    "\n",
    "# think of how to set those variables so that variables from different tweets are not learned from together!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrones2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-13 19:17:31,020 : INFO : collecting all words and their counts\n",
      "2019-01-13 19:17:31,021 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-01-13 19:17:31,029 : INFO : PROGRESS: at sentence #10000, processed 39369 words, keeping 885 word types\n",
      "2019-01-13 19:17:31,037 : INFO : PROGRESS: at sentence #20000, processed 81087 words, keeping 981 word types\n",
      "2019-01-13 19:17:31,045 : INFO : PROGRESS: at sentence #30000, processed 120114 words, keeping 1018 word types\n",
      "2019-01-13 19:17:31,053 : INFO : PROGRESS: at sentence #40000, processed 160613 words, keeping 1054 word types\n",
      "2019-01-13 19:17:31,061 : INFO : PROGRESS: at sentence #50000, processed 200972 words, keeping 1074 word types\n",
      "2019-01-13 19:17:31,069 : INFO : PROGRESS: at sentence #60000, processed 239581 words, keeping 1091 word types\n",
      "2019-01-13 19:17:31,077 : INFO : PROGRESS: at sentence #70000, processed 278164 words, keeping 1105 word types\n",
      "2019-01-13 19:17:31,086 : INFO : PROGRESS: at sentence #80000, processed 318013 words, keeping 1110 word types\n",
      "2019-01-13 19:17:31,087 : INFO : collected 1111 word types from a corpus of 321470 raw words and 80899 sentences\n",
      "2019-01-13 19:17:31,088 : INFO : Loading a fresh vocabulary\n",
      "2019-01-13 19:17:31,089 : INFO : effective_min_count=3 retains 1002 unique words (90% of original 1111, drops 109)\n",
      "2019-01-13 19:17:31,090 : INFO : effective_min_count=3 leaves 321318 word corpus (99% of original 321470, drops 152)\n",
      "2019-01-13 19:17:31,092 : INFO : deleting the raw counts dictionary of 1111 items\n",
      "2019-01-13 19:17:31,093 : INFO : sample=0.001 downsamples 82 most-common words\n",
      "2019-01-13 19:17:31,093 : INFO : downsampling leaves estimated 196722 word corpus (61.2% of prior 321318)\n",
      "2019-01-13 19:17:31,095 : INFO : estimated required memory for 1002 words and 300 dimensions: 2905800 bytes\n",
      "2019-01-13 19:17:31,096 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "thrones2vec.build_vocab(emojiSequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-13 19:17:31,116 : INFO : training model with 8 workers on 1002 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=2\n",
      "2019-01-13 19:17:31,266 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-13 19:17:31,268 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-13 19:17:31,270 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-13 19:17:31,271 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-13 19:17:31,273 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-13 19:17:31,276 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-13 19:17:31,278 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-13 19:17:31,279 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-13 19:17:31,280 : INFO : EPOCH - 1 : training on 321470 raw words (196750 effective words) took 0.1s, 1390582 effective words/s\n",
      "2019-01-13 19:17:31,420 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-13 19:17:31,421 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-13 19:17:31,422 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-13 19:17:31,429 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-13 19:17:31,431 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-13 19:17:31,432 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-13 19:17:31,432 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-13 19:17:31,433 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-13 19:17:31,433 : INFO : EPOCH - 2 : training on 321470 raw words (196699 effective words) took 0.1s, 1525426 effective words/s\n",
      "2019-01-13 19:17:31,434 : INFO : training on a 642940 raw words (393449 effective words) took 0.3s, 1242844 effective words/s\n",
      "2019-01-13 19:17:31,434 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(393449, 642940)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thrones2vec.train(emojiSequences, total_examples=thrones2vec.corpus_count, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-13 19:17:31,444 : INFO : saving Word2Vec object under trained/2nd.w2v, separately None\n",
      "2019-01-13 19:17:31,445 : INFO : not storing attribute vectors_norm\n",
      "2019-01-13 19:17:31,446 : INFO : not storing attribute cum_table\n",
      "2019-01-13 19:17:31,467 : INFO : saved trained/2nd.w2v\n"
     ]
    }
   ],
   "source": [
    "thrones2vec.save(os.path.join(\"trained\", \"2nd.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explore the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-13 19:17:31,471 : INFO : loading Word2Vec object from trained/2nd.w2v\n",
      "2019-01-13 19:17:31,486 : INFO : loading wv recursively from trained/2nd.w2v.wv.* with mmap=None\n",
      "2019-01-13 19:17:31,486 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-01-13 19:17:31,487 : INFO : loading vocabulary recursively from trained/2nd.w2v.vocabulary.* with mmap=None\n",
      "2019-01-13 19:17:31,487 : INFO : loading trainables recursively from trained/2nd.w2v.trainables.* with mmap=None\n",
      "2019-01-13 19:17:31,488 : INFO : setting ignored attribute cum_table to None\n",
      "2019-01-13 19:17:31,488 : INFO : loaded trained/2nd.w2v\n"
     ]
    }
   ],
   "source": [
    "thrones2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"2nd.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cosine similarity between ğŸ‡¬ğŸ‡§ and ğŸ‡ºğŸ‡¸ could not be computed.\n",
      "the cosine similarity between ğŸ†“ and ğŸ’¸ could not be computed.\n",
      "the cosine similarity between ğŸ… and ğŸ‡ºğŸ‡¸ could not be computed.\n",
      "the cosine similarity between ğŸ†“ and ğŸ’ƒ could not be computed.\n",
      "the cosine similarity between ğŸ‡ºğŸ‡¸ and â¤ could not be computed.\n",
      "the cosine similarity between ğŸŒƒ and ğŸ•¹ could not be computed.\n",
      "the cosine similarity between ğŸ†“ and ğŸ“ could not be computed.\n",
      "the cosine similarity between ğŸŒƒ and ğŸ†“ could not be computed.\n",
      "the cosine similarity between ğŸš« and ğŸ†“ could not be computed.\n",
      "the cosine similarity between ğŸ˜ and ğŸ•¹ could not be computed.\n",
      "the cosine similarity between ğŸ‡ºğŸ‡¸ and ğŸ’¥ could not be computed.\n",
      "the cosine similarity between ğŸ¤ and ğŸ‡³ğŸ‡¬ could not be computed.\n",
      "the cosine similarity between ğŸ•¹ and ğŸ’¯ could not be computed.\n",
      "the cosine similarity between ğŸ‡³ğŸ‡¬ and ğŸ“² could not be computed.\n",
      "the cosine similarity between ğŸ‘‡ and ğŸ‡³ğŸ‡¬ could not be computed.\n",
      "the cosine similarity between ğŸ§ and ğŸ‡³ğŸ‡¬ could not be computed.\n",
      "the cosine similarity between â­ and ğŸ‘ could not be computed.\n",
      "the cosine similarity between ğŸ‡³ğŸ‡¬ and ğŸ¶ could not be computed.\n",
      "the cosine similarity between ğŸ‘ and â†ª could not be computed.\n",
      "Der Spearman Rank Correlation Coefficient is SpearmanrResult(correlation=-0.0796094023782141, pvalue=0.07862559963359836)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# extracts the 508 Annotator Results as the Gold-Standard\n",
    "corpus_filename = '../data/EmoSim508.json'\n",
    "corpus = open(corpus_filename).read()\n",
    "annotator_similarity_score_508 = list(array(re.findall('(?<=_Annotator_Agreement\": )(.*?)(?=\\})', corpus)))\n",
    "\n",
    "# glyph_pairs_1016\n",
    "unicode_pairs_1016 = re.findall('(?<=unicodelong\": \"\\\\\\)(.*?)(?=\")', corpus)    \n",
    "glyph_pairs_1016 = [codecs.decode(unicode_pairs_1016[x].replace(str('\\\\\\\\'),str('\\\\')).replace('_',''), 'unicode_escape') for x in range(len(unicode_pairs_1016))]\n",
    "\n",
    "# computation of SpearRank\n",
    "goldstandard = []\n",
    "selftrained = []\n",
    "for x in range(len(annotator_similarity_score_508)):\n",
    "    cosineSimilarity = None\n",
    "    \n",
    "    emoji1 = glyph_pairs_1016.pop(0)\n",
    "    emoji2 = glyph_pairs_1016.pop(0)\n",
    "    \n",
    "    try:\n",
    "        cosineSimilarity = thrones2vec.wv.similarity(emoji1, emoji2)\n",
    "    except:\n",
    "        print('the cosine similarity between ' + emoji1 + ' and ' + emoji2 + ' could not be computed.')\n",
    "    \n",
    "    if(cosineSimilarity is not None):\n",
    "        selftrained.append(cosineSimilarity)\n",
    "        goldstandard.append(annotator_similarity_score_508.pop(0))\n",
    "\n",
    "spearmanRank = stats.spearmanr(goldstandard, selftrained)\n",
    "\n",
    "print('Der Spearman Rank Correlation Coefficient is {}'.format(spearmanRank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
