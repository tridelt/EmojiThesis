{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy  import array\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['seed']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_filename = '../data/extracted_emoji_sequences.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(corpus_filename).read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlyEmojiSequences(tokens):\n",
    "    threshold_emojis = [x for x in tokens if len(x) > 1]\n",
    "    return threshold_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "emojiSequences = onlyEmojiSequences(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 610,256 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in emojiSequences])\n",
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality of the resulting word vectors.\n",
    "num_features = 300\n",
    "\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 2\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "# remove later\n",
    "seed = 1\n",
    "\n",
    "# think of how to set those variables so that variables from different tweets are not learned from together!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrones2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-13 19:17:31,020 : INFO : collecting all words and their counts\n",
      "2019-01-13 19:17:31,021 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-01-13 19:17:31,029 : INFO : PROGRESS: at sentence #10000, processed 39369 words, keeping 885 word types\n",
      "2019-01-13 19:17:31,037 : INFO : PROGRESS: at sentence #20000, processed 81087 words, keeping 981 word types\n",
      "2019-01-13 19:17:31,045 : INFO : PROGRESS: at sentence #30000, processed 120114 words, keeping 1018 word types\n",
      "2019-01-13 19:17:31,053 : INFO : PROGRESS: at sentence #40000, processed 160613 words, keeping 1054 word types\n",
      "2019-01-13 19:17:31,061 : INFO : PROGRESS: at sentence #50000, processed 200972 words, keeping 1074 word types\n",
      "2019-01-13 19:17:31,069 : INFO : PROGRESS: at sentence #60000, processed 239581 words, keeping 1091 word types\n",
      "2019-01-13 19:17:31,077 : INFO : PROGRESS: at sentence #70000, processed 278164 words, keeping 1105 word types\n",
      "2019-01-13 19:17:31,086 : INFO : PROGRESS: at sentence #80000, processed 318013 words, keeping 1110 word types\n",
      "2019-01-13 19:17:31,087 : INFO : collected 1111 word types from a corpus of 321470 raw words and 80899 sentences\n",
      "2019-01-13 19:17:31,088 : INFO : Loading a fresh vocabulary\n",
      "2019-01-13 19:17:31,089 : INFO : effective_min_count=3 retains 1002 unique words (90% of original 1111, drops 109)\n",
      "2019-01-13 19:17:31,090 : INFO : effective_min_count=3 leaves 321318 word corpus (99% of original 321470, drops 152)\n",
      "2019-01-13 19:17:31,092 : INFO : deleting the raw counts dictionary of 1111 items\n",
      "2019-01-13 19:17:31,093 : INFO : sample=0.001 downsamples 82 most-common words\n",
      "2019-01-13 19:17:31,093 : INFO : downsampling leaves estimated 196722 word corpus (61.2% of prior 321318)\n",
      "2019-01-13 19:17:31,095 : INFO : estimated required memory for 1002 words and 300 dimensions: 2905800 bytes\n",
      "2019-01-13 19:17:31,096 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "thrones2vec.build_vocab(emojiSequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-13 19:17:31,116 : INFO : training model with 8 workers on 1002 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=2\n",
      "2019-01-13 19:17:31,266 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-13 19:17:31,268 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-13 19:17:31,270 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-13 19:17:31,271 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-13 19:17:31,273 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-13 19:17:31,276 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-13 19:17:31,278 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-13 19:17:31,279 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-13 19:17:31,280 : INFO : EPOCH - 1 : training on 321470 raw words (196750 effective words) took 0.1s, 1390582 effective words/s\n",
      "2019-01-13 19:17:31,420 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-13 19:17:31,421 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-13 19:17:31,422 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-13 19:17:31,429 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-13 19:17:31,431 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-13 19:17:31,432 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-13 19:17:31,432 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-13 19:17:31,433 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-13 19:17:31,433 : INFO : EPOCH - 2 : training on 321470 raw words (196699 effective words) took 0.1s, 1525426 effective words/s\n",
      "2019-01-13 19:17:31,434 : INFO : training on a 642940 raw words (393449 effective words) took 0.3s, 1242844 effective words/s\n",
      "2019-01-13 19:17:31,434 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(393449, 642940)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thrones2vec.train(emojiSequences, total_examples=thrones2vec.corpus_count, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-13 19:17:31,444 : INFO : saving Word2Vec object under trained/2nd.w2v, separately None\n",
      "2019-01-13 19:17:31,445 : INFO : not storing attribute vectors_norm\n",
      "2019-01-13 19:17:31,446 : INFO : not storing attribute cum_table\n",
      "2019-01-13 19:17:31,467 : INFO : saved trained/2nd.w2v\n"
     ]
    }
   ],
   "source": [
    "thrones2vec.save(os.path.join(\"trained\", \"2nd.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explore the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-13 19:17:31,471 : INFO : loading Word2Vec object from trained/2nd.w2v\n",
      "2019-01-13 19:17:31,486 : INFO : loading wv recursively from trained/2nd.w2v.wv.* with mmap=None\n",
      "2019-01-13 19:17:31,486 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-01-13 19:17:31,487 : INFO : loading vocabulary recursively from trained/2nd.w2v.vocabulary.* with mmap=None\n",
      "2019-01-13 19:17:31,487 : INFO : loading trainables recursively from trained/2nd.w2v.trainables.* with mmap=None\n",
      "2019-01-13 19:17:31,488 : INFO : setting ignored attribute cum_table to None\n",
      "2019-01-13 19:17:31,488 : INFO : loaded trained/2nd.w2v\n"
     ]
    }
   ],
   "source": [
    "thrones2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"2nd.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cosine similarity between 🇬🇧 and 🇺🇸 could not be computed.\n",
      "the cosine similarity between 🆓 and 💸 could not be computed.\n",
      "the cosine similarity between 🏅 and 🇺🇸 could not be computed.\n",
      "the cosine similarity between 🆓 and 💃 could not be computed.\n",
      "the cosine similarity between 🇺🇸 and ❤ could not be computed.\n",
      "the cosine similarity between 🌃 and 🕹 could not be computed.\n",
      "the cosine similarity between 🆓 and 📍 could not be computed.\n",
      "the cosine similarity between 🌃 and 🆓 could not be computed.\n",
      "the cosine similarity between 🚫 and 🆓 could not be computed.\n",
      "the cosine similarity between 😏 and 🕹 could not be computed.\n",
      "the cosine similarity between 🇺🇸 and 💥 could not be computed.\n",
      "the cosine similarity between 🎤 and 🇳🇬 could not be computed.\n",
      "the cosine similarity between 🕹 and 💯 could not be computed.\n",
      "the cosine similarity between 🇳🇬 and 📲 could not be computed.\n",
      "the cosine similarity between 👇 and 🇳🇬 could not be computed.\n",
      "the cosine similarity between 🎧 and 🇳🇬 could not be computed.\n",
      "the cosine similarity between ⏭ and 👏 could not be computed.\n",
      "the cosine similarity between 🇳🇬 and 🎶 could not be computed.\n",
      "the cosine similarity between 👏 and ↪ could not be computed.\n",
      "Der Spearman Rank Correlation Coefficient is SpearmanrResult(correlation=-0.0796094023782141, pvalue=0.07862559963359836)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# extracts the 508 Annotator Results as the Gold-Standard\n",
    "corpus_filename = '../data/EmoSim508.json'\n",
    "corpus = open(corpus_filename).read()\n",
    "annotator_similarity_score_508 = list(array(re.findall('(?<=_Annotator_Agreement\": )(.*?)(?=\\})', corpus)))\n",
    "\n",
    "# glyph_pairs_1016\n",
    "unicode_pairs_1016 = re.findall('(?<=unicodelong\": \"\\\\\\)(.*?)(?=\")', corpus)    \n",
    "glyph_pairs_1016 = [codecs.decode(unicode_pairs_1016[x].replace(str('\\\\\\\\'),str('\\\\')).replace('_',''), 'unicode_escape') for x in range(len(unicode_pairs_1016))]\n",
    "\n",
    "# computation of SpearRank\n",
    "goldstandard = []\n",
    "selftrained = []\n",
    "for x in range(len(annotator_similarity_score_508)):\n",
    "    cosineSimilarity = None\n",
    "    \n",
    "    emoji1 = glyph_pairs_1016.pop(0)\n",
    "    emoji2 = glyph_pairs_1016.pop(0)\n",
    "    \n",
    "    try:\n",
    "        cosineSimilarity = thrones2vec.wv.similarity(emoji1, emoji2)\n",
    "    except:\n",
    "        print('the cosine similarity between ' + emoji1 + ' and ' + emoji2 + ' could not be computed.')\n",
    "    \n",
    "    if(cosineSimilarity is not None):\n",
    "        selftrained.append(cosineSimilarity)\n",
    "        goldstandard.append(annotator_similarity_score_508.pop(0))\n",
    "\n",
    "spearmanRank = stats.spearmanr(goldstandard, selftrained)\n",
    "\n",
    "print('Der Spearman Rank Correlation Coefficient is {}'.format(spearmanRank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
