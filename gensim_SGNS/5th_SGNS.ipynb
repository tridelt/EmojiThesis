{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from numpy  import array\n",
    "from scipy import stats\n",
    "from gensim.models.callbacks import CallbackAny2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_filename = '../data/extracted_emoji_sequences.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(corpus_filename).read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlyEmojiSequences(tokens):\n",
    "    threshold_emojis = [x for x in tokens if len(x) > 1]\n",
    "    return threshold_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "emojiSequences = onlyEmojiSequences(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus contains 610,256 emoji points.\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in emojiSequences])\n",
    "print(\"The corpus contains {0:,} emoji points.\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETTING VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality of the resulting word vectors.\n",
    "num_features = 300\n",
    "\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "# num_workers = 1\n",
    "\n",
    "\n",
    "# Context window length.\n",
    "context_size = 4\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "# remove later\n",
    "seed = 1\n",
    "\n",
    "emoji2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.tempLoss = 0\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{}  Loss-Value: {}\".format(self.epoch, (emoji2vec.get_latest_training_loss() - self.tempLoss)))\n",
    "        self.epoch += 1\n",
    "        self.tempLoss = emoji2vec.get_latest_training_loss()\n",
    "        \n",
    "epoch_logger = EpochLogger()\n",
    "emoji2vec.build_vocab(emojiSequences, progress_per=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0  Loss-Value: 137791.21875\n",
      "Epoch #1  Loss-Value: 85361.34375\n",
      "Epoch #2  Loss-Value: 110157.34375\n",
      "Epoch #3  Loss-Value: 97510.09375\n",
      "Epoch #4  Loss-Value: 85497.3125\n",
      "Epoch #5  Loss-Value: 82586.8125\n",
      "Epoch #6  Loss-Value: 71961.8125\n",
      "Epoch #7  Loss-Value: 79175.6875\n",
      "Epoch #8  Loss-Value: 93126.9375\n",
      "Epoch #9  Loss-Value: 86470.625\n",
      "Epoch #10  Loss-Value: 81008.5625\n",
      "Epoch #11  Loss-Value: 78850.25\n",
      "Epoch #12  Loss-Value: 92518.25\n",
      "Epoch #13  Loss-Value: 78041.125\n",
      "Epoch #14  Loss-Value: 75607.75\n",
      "Epoch #15  Loss-Value: 64812.875\n",
      "Epoch #16  Loss-Value: 77415.125\n",
      "Epoch #17  Loss-Value: 66904.125\n",
      "Epoch #18  Loss-Value: 71637.875\n",
      "Epoch #19  Loss-Value: 72455.375\n",
      "Epoch #20  Loss-Value: 74825.125\n",
      "Epoch #21  Loss-Value: 79501.0\n",
      "Epoch #22  Loss-Value: 79164.5\n",
      "Epoch #23  Loss-Value: 73902.5\n",
      "Epoch #24  Loss-Value: 75558.375\n",
      "Epoch #25  Loss-Value: 80786.75\n",
      "Epoch #26  Loss-Value: 84774.25\n",
      "Epoch #27  Loss-Value: 63922.5\n",
      "Epoch #28  Loss-Value: 79881.5\n",
      "Epoch #29  Loss-Value: 75832.5\n",
      "Epoch #30  Loss-Value: 82533.0\n",
      "Epoch #31  Loss-Value: 66024.0\n",
      "Epoch #32  Loss-Value: 72029.25\n",
      "Epoch #33  Loss-Value: 78431.5\n",
      "Epoch #34  Loss-Value: 79548.0\n",
      "Epoch #35  Loss-Value: 82503.25\n",
      "Epoch #36  Loss-Value: 64294.25\n",
      "Epoch #37  Loss-Value: 70882.25\n",
      "Epoch #38  Loss-Value: 82015.75\n",
      "Epoch #39  Loss-Value: 71659.0\n",
      "Epoch #40  Loss-Value: 92313.0\n",
      "Epoch #41  Loss-Value: 71027.0\n",
      "Epoch #42  Loss-Value: 63111.75\n",
      "Epoch #43  Loss-Value: 69247.25\n",
      "Epoch #44  Loss-Value: 78141.25\n",
      "Epoch #45  Loss-Value: 78233.5\n",
      "Epoch #46  Loss-Value: 60477.0\n",
      "Epoch #47  Loss-Value: 78691.25\n",
      "Epoch #48  Loss-Value: 74472.0\n",
      "Epoch #49  Loss-Value: 72924.25\n",
      "Epoch #50  Loss-Value: 86253.5\n",
      "Epoch #51  Loss-Value: 91460.75\n",
      "Epoch #52  Loss-Value: 80636.75\n",
      "Epoch #53  Loss-Value: 79519.0\n",
      "Epoch #54  Loss-Value: 68441.0\n",
      "Epoch #55  Loss-Value: 58026.5\n",
      "Epoch #56  Loss-Value: 70720.5\n",
      "Epoch #57  Loss-Value: 66282.0\n",
      "Epoch #58  Loss-Value: 69046.5\n",
      "Epoch #59  Loss-Value: 79018.0\n",
      "Epoch #60  Loss-Value: 66204.0\n",
      "Epoch #61  Loss-Value: 78224.5\n",
      "Epoch #62  Loss-Value: 69690.0\n",
      "Epoch #63  Loss-Value: 86050.5\n",
      "Epoch #64  Loss-Value: 76031.5\n",
      "Epoch #65  Loss-Value: 67317.0\n",
      "Epoch #66  Loss-Value: 82549.5\n",
      "Epoch #67  Loss-Value: 67475.0\n",
      "Epoch #68  Loss-Value: 69105.5\n",
      "Epoch #69  Loss-Value: 65320.5\n",
      "Epoch #70  Loss-Value: 76572.5\n",
      "Epoch #71  Loss-Value: 67149.5\n",
      "Epoch #72  Loss-Value: 75734.5\n",
      "Epoch #73  Loss-Value: 61308.5\n",
      "Epoch #74  Loss-Value: 68607.0\n",
      "Epoch #75  Loss-Value: 76446.0\n",
      "Epoch #76  Loss-Value: 69119.0\n",
      "Epoch #77  Loss-Value: 66070.0\n",
      "Epoch #78  Loss-Value: 63147.5\n",
      "Epoch #79  Loss-Value: 70667.0\n",
      "Epoch #80  Loss-Value: 65864.5\n",
      "Epoch #81  Loss-Value: 65998.0\n",
      "Epoch #82  Loss-Value: 67690.5\n",
      "Epoch #83  Loss-Value: 57941.5\n",
      "Epoch #84  Loss-Value: 80120.0\n",
      "Epoch #85  Loss-Value: 77900.5\n",
      "Epoch #86  Loss-Value: 61371.0\n",
      "Epoch #87  Loss-Value: 59959.5\n",
      "Epoch #88  Loss-Value: 67374.0\n",
      "Epoch #89  Loss-Value: 68191.5\n",
      "Epoch #90  Loss-Value: 78592.5\n",
      "Epoch #91  Loss-Value: 64671.0\n",
      "Epoch #92  Loss-Value: 56390.5\n",
      "Epoch #93  Loss-Value: 66044.5\n",
      "Epoch #94  Loss-Value: 73550.5\n",
      "Epoch #95  Loss-Value: 58399.5\n",
      "Epoch #96  Loss-Value: 74132.5\n",
      "Epoch #97  Loss-Value: 75770.0\n",
      "Epoch #98  Loss-Value: 73782.0\n",
      "Epoch #99  Loss-Value: 64516.0\n",
      "Epoch #100  Loss-Value: 76204.0\n",
      "Epoch #101  Loss-Value: 73721.0\n",
      "Epoch #102  Loss-Value: 79385.5\n",
      "Epoch #103  Loss-Value: 71074.5\n",
      "Epoch #104  Loss-Value: 68170.0\n",
      "Epoch #105  Loss-Value: 56117.0\n",
      "Epoch #106  Loss-Value: 65613.0\n",
      "Epoch #107  Loss-Value: 65544.5\n",
      "Epoch #108  Loss-Value: 66515.5\n",
      "Epoch #109  Loss-Value: 60319.0\n",
      "Epoch #110  Loss-Value: 66375.5\n",
      "Epoch #111  Loss-Value: 68698.5\n",
      "Epoch #112  Loss-Value: 68313.0\n",
      "Epoch #113  Loss-Value: 72146.0\n",
      "Epoch #114  Loss-Value: 68785.0\n",
      "Epoch #115  Loss-Value: 60841.0\n",
      "Epoch #116  Loss-Value: 53493.0\n",
      "Epoch #117  Loss-Value: 60686.0\n",
      "Epoch #118  Loss-Value: 60028.0\n",
      "Epoch #119  Loss-Value: 57461.0\n",
      "Epoch #120  Loss-Value: 67086.0\n",
      "Epoch #121  Loss-Value: 62705.0\n",
      "Epoch #122  Loss-Value: 80760.0\n",
      "Epoch #123  Loss-Value: 61828.0\n",
      "Epoch #124  Loss-Value: 63870.0\n",
      "Epoch #125  Loss-Value: 66827.0\n",
      "Epoch #126  Loss-Value: 62922.0\n",
      "Epoch #127  Loss-Value: 59337.0\n",
      "Epoch #128  Loss-Value: 61416.0\n",
      "Epoch #129  Loss-Value: 62806.0\n",
      "Epoch #130  Loss-Value: 65922.0\n",
      "Epoch #131  Loss-Value: 58855.0\n",
      "Epoch #132  Loss-Value: 66358.0\n",
      "Epoch #133  Loss-Value: 53040.0\n",
      "Epoch #134  Loss-Value: 69815.0\n",
      "Epoch #135  Loss-Value: 58994.0\n",
      "Epoch #136  Loss-Value: 57160.0\n",
      "Epoch #137  Loss-Value: 58390.0\n",
      "Epoch #138  Loss-Value: 52638.0\n",
      "Epoch #139  Loss-Value: 55130.0\n",
      "Epoch #140  Loss-Value: 65750.0\n",
      "Epoch #141  Loss-Value: 68201.0\n",
      "Epoch #142  Loss-Value: 74453.0\n",
      "Epoch #143  Loss-Value: 70662.0\n",
      "Epoch #144  Loss-Value: 56932.0\n",
      "Epoch #145  Loss-Value: 59572.0\n",
      "Epoch #146  Loss-Value: 63173.0\n",
      "Epoch #147  Loss-Value: 50749.0\n",
      "Epoch #148  Loss-Value: 69226.0\n",
      "Epoch #149  Loss-Value: 58482.0\n",
      "Epoch #150  Loss-Value: 67957.0\n",
      "Epoch #151  Loss-Value: 79913.0\n",
      "Epoch #152  Loss-Value: 71492.0\n",
      "Epoch #153  Loss-Value: 51253.0\n",
      "Epoch #154  Loss-Value: 62162.0\n",
      "Epoch #155  Loss-Value: 51761.0\n",
      "Epoch #156  Loss-Value: 65537.0\n",
      "Epoch #157  Loss-Value: 55133.0\n",
      "Epoch #158  Loss-Value: 58702.0\n",
      "Epoch #159  Loss-Value: 67538.0\n",
      "Epoch #160  Loss-Value: 60810.0\n",
      "Epoch #161  Loss-Value: 62214.0\n",
      "Epoch #162  Loss-Value: 51453.0\n",
      "Epoch #163  Loss-Value: 65118.0\n",
      "Epoch #164  Loss-Value: 52188.0\n",
      "Epoch #165  Loss-Value: 65299.0\n",
      "Epoch #166  Loss-Value: 71421.0\n",
      "Epoch #167  Loss-Value: 58265.0\n",
      "Epoch #168  Loss-Value: 67318.0\n",
      "Epoch #169  Loss-Value: 57242.0\n",
      "Epoch #170  Loss-Value: 58325.0\n",
      "Epoch #171  Loss-Value: 53266.0\n",
      "Epoch #172  Loss-Value: 68016.0\n",
      "Epoch #173  Loss-Value: 57250.0\n",
      "Epoch #174  Loss-Value: 60791.0\n",
      "Epoch #175  Loss-Value: 61188.0\n",
      "Epoch #176  Loss-Value: 51115.0\n",
      "Epoch #177  Loss-Value: 61617.0\n",
      "Epoch #178  Loss-Value: 53017.0\n",
      "Epoch #179  Loss-Value: 52856.0\n",
      "Epoch #180  Loss-Value: 58201.0\n",
      "Epoch #181  Loss-Value: 69227.0\n",
      "Epoch #182  Loss-Value: 67669.0\n",
      "Epoch #183  Loss-Value: 59725.0\n",
      "Epoch #184  Loss-Value: 68735.0\n",
      "Epoch #185  Loss-Value: 69250.0\n",
      "Epoch #186  Loss-Value: 59126.0\n",
      "Epoch #187  Loss-Value: 66992.0\n",
      "Epoch #188  Loss-Value: 51860.0\n",
      "Epoch #189  Loss-Value: 68376.0\n",
      "Epoch #190  Loss-Value: 66372.0\n",
      "Epoch #191  Loss-Value: 69798.0\n",
      "Epoch #192  Loss-Value: 60151.0\n",
      "Epoch #193  Loss-Value: 68449.0\n",
      "Epoch #194  Loss-Value: 50529.0\n",
      "Epoch #195  Loss-Value: 53678.0\n",
      "Epoch #196  Loss-Value: 61936.0\n",
      "Epoch #197  Loss-Value: 60133.0\n",
      "Epoch #198  Loss-Value: 50789.0\n",
      "Epoch #199  Loss-Value: 58386.0\n",
      "Epoch #200  Loss-Value: 71635.0\n",
      "Epoch #201  Loss-Value: 51407.0\n",
      "Epoch #202  Loss-Value: 53275.0\n",
      "Epoch #203  Loss-Value: 51379.0\n",
      "Epoch #204  Loss-Value: 52225.0\n",
      "Epoch #205  Loss-Value: 67368.0\n",
      "Epoch #206  Loss-Value: 57775.0\n",
      "Epoch #207  Loss-Value: 67094.0\n",
      "Epoch #208  Loss-Value: 66460.0\n",
      "Epoch #209  Loss-Value: 70530.0\n"
     ]
    }
   ],
   "source": [
    "emoji2vec.train(emojiSequences, total_examples=emoji2vec.corpus_count, epochs = 10, compute_loss=True, callbacks=[epoch_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")\n",
    "emoji2vec.save(os.path.join(\"trained\", \"canIseed.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATING TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"canIseed.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cosine similarity between üá¨üáß and üá∫üá∏ could not be computed.\n",
      "the cosine similarity between üèÖ and üá∫üá∏ could not be computed.\n",
      "the cosine similarity between üá∫üá∏ and ‚ù§ could not be computed.\n",
      "the cosine similarity between üá∫üá∏ and üí• could not be computed.\n",
      "the cosine similarity between üé§ and üá≥üá¨ could not be computed.\n",
      "the cosine similarity between üá≥üá¨ and üì≤ could not be computed.\n",
      "the cosine similarity between üëá and üá≥üá¨ could not be computed.\n",
      "the cosine similarity between üéß and üá≥üá¨ could not be computed.\n",
      "the cosine similarity between üá≥üá¨ and üé∂ could not be computed.\n",
      "the cosine similarity between üëè and ‚Ü™ could not be computed.\n",
      "Der Spearman Rank Correlation Coefficient is SpearmanrResult(correlation=0.520770001820602, pvalue=5.776830608808171e-36)\n"
     ]
    }
   ],
   "source": [
    "# extracts the 508 Annotator Results as the Gold-Standard\n",
    "corpus_filename = '../data/EmoSim508.json'\n",
    "corpus = open(corpus_filename).read()\n",
    "annotator_similarity_score_508 = list(array(re.findall('(?<=_Annotator_Agreement\": )(.*?)(?=\\})', corpus)))\n",
    "\n",
    "# glyph_pairs_1016\n",
    "unicode_pairs_1016 = re.findall('(?<=unicodelong\": \"\\\\\\)(.*?)(?=\")', corpus)    \n",
    "glyph_pairs_1016 = [codecs.decode(unicode_pairs_1016[x].replace(str('\\\\\\\\'),str('\\\\')).replace('_',''), 'unicode_escape') for x in range(len(unicode_pairs_1016))]\n",
    "\n",
    "# computation of Cosine Similarity\n",
    "goldstandard = []\n",
    "selftrained = []\n",
    "for x in range(len(annotator_similarity_score_508)):\n",
    "    cosineSimilarity = None\n",
    "    \n",
    "    emoji1 = glyph_pairs_1016.pop(0)\n",
    "    emoji2 = glyph_pairs_1016.pop(0)\n",
    "    \n",
    "    try:\n",
    "        cosineSimilarity = emoji2vec.wv.similarity(emoji1, emoji2)\n",
    "    except:\n",
    "        print('the cosine similarity between ' + emoji1 + ' and ' + emoji2 + ' could not be computed.')\n",
    "    \n",
    "    if(cosineSimilarity is not None):\n",
    "        selftrained.append(cosineSimilarity)\n",
    "        goldstandard.append(annotator_similarity_score_508.pop(0))\n",
    "\n",
    "# computation of SPEARRANK CORRELATION COEFFICIENT\n",
    "spearmanRank = stats.spearmanr(goldstandard, selftrained)\n",
    "\n",
    "print('Der Spearman Rank Correlation Coefficient is {}'.format(spearmanRank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emoji2vec.wv.get_vector('üôÅ')\n",
    "# emoji2vec.wv.similarity('üôÅ', 'ü§í')\n",
    "# emoji2vec.wv.most_similar('üí¶')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
