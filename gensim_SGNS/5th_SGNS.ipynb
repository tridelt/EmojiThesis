{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from numpy  import array\n",
    "from scipy import stats\n",
    "from gensim.models.callbacks import CallbackAny2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_filename = '../data/extracted_emoji_sequences.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(corpus_filename).read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlyEmojiSequences(tokens):\n",
    "    threshold_emojis = [x for x in tokens if len(x) > 1]\n",
    "    return threshold_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "emojiSequences = onlyEmojiSequences(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 610,256 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in emojiSequences])\n",
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETTING VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality of the resulting word vectors.\n",
    "num_features = 300\n",
    "\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 2\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "# remove later\n",
    "seed = 1\n",
    "\n",
    "emoji2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{}  Loss-Value: {}\".format(self.epoch, emoji2vec.get_latest_training_loss()))\n",
    "        self.epoch += 1\n",
    "        \n",
    "epoch_logger = EpochLogger()\n",
    "\n",
    "emoji2vec.build_vocab(emojiSequences, progress_per=10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 . Loss-Value: 142701.515625\n",
      "Epoch #1 . Loss-Value: 227239.5625\n",
      "Epoch #2 . Loss-Value: 304366.78125\n",
      "Epoch #3 . Loss-Value: 388530.40625\n",
      "Epoch #4 . Loss-Value: 458362.96875\n",
      "Epoch #5 . Loss-Value: 534406.375\n",
      "Epoch #6 . Loss-Value: 606288.25\n",
      "Epoch #7 . Loss-Value: 686856.4375\n",
      "Epoch #8 . Loss-Value: 749028.125\n",
      "Epoch #9 . Loss-Value: 827810.875\n",
      "Epoch #10 . Loss-Value: 894664.0\n",
      "Epoch #11 . Loss-Value: 961246.5\n",
      "Epoch #12 . Loss-Value: 1035710.1875\n",
      "Epoch #13 . Loss-Value: 1102007.25\n",
      "Epoch #14 . Loss-Value: 1165470.125\n",
      "Epoch #15 . Loss-Value: 1238970.625\n",
      "Epoch #16 . Loss-Value: 1310930.625\n",
      "Epoch #17 . Loss-Value: 1386424.25\n",
      "Epoch #18 . Loss-Value: 1457585.5\n",
      "Epoch #19 . Loss-Value: 1525235.75\n",
      "Epoch #20 . Loss-Value: 1597838.875\n",
      "Epoch #21 . Loss-Value: 1660244.625\n",
      "Epoch #22 . Loss-Value: 1722467.375\n",
      "Epoch #23 . Loss-Value: 1784240.375\n",
      "Epoch #24 . Loss-Value: 1846862.125\n",
      "Epoch #25 . Loss-Value: 1919028.625\n",
      "Epoch #26 . Loss-Value: 1979662.5\n",
      "Epoch #27 . Loss-Value: 2050654.625\n",
      "Epoch #28 . Loss-Value: 2122983.25\n",
      "Epoch #29 . Loss-Value: 2178572.25\n",
      "Epoch #30 . Loss-Value: 2231984.75\n",
      "Epoch #31 . Loss-Value: 2302577.0\n",
      "Epoch #32 . Loss-Value: 2362083.75\n",
      "Epoch #33 . Loss-Value: 2422503.0\n",
      "Epoch #34 . Loss-Value: 2493647.5\n",
      "Epoch #35 . Loss-Value: 2554187.5\n",
      "Epoch #36 . Loss-Value: 2615831.0\n",
      "Epoch #37 . Loss-Value: 2673082.25\n",
      "Epoch #38 . Loss-Value: 2738984.75\n",
      "Epoch #39 . Loss-Value: 2798676.75\n",
      "Epoch #40 . Loss-Value: 2857421.0\n",
      "Epoch #41 . Loss-Value: 2919398.25\n",
      "Epoch #42 . Loss-Value: 2980178.75\n",
      "Epoch #43 . Loss-Value: 3042993.25\n",
      "Epoch #44 . Loss-Value: 3111027.0\n",
      "Epoch #45 . Loss-Value: 3173397.75\n",
      "Epoch #46 . Loss-Value: 3247952.5\n",
      "Epoch #47 . Loss-Value: 3315300.5\n",
      "Epoch #48 . Loss-Value: 3383617.0\n",
      "Epoch #49 . Loss-Value: 3451345.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19443895, 30512800)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji2vec.train(emojiSequences, total_examples=emoji2vec.corpus_count, epochs = 50, compute_loss=True, callbacks=[epoch_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji2vec.save(os.path.join(\"trained\", \"2nd.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATING TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"2nd.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cosine similarity between üá¨üáß and üá∫üá∏ could not be computed.\n",
      "the cosine similarity between üèÖ and üá∫üá∏ could not be computed.\n",
      "the cosine similarity between üá∫üá∏ and ‚ù§ could not be computed.\n",
      "the cosine similarity between üá∫üá∏ and üí• could not be computed.\n",
      "the cosine similarity between üé§ and üá≥üá¨ could not be computed.\n",
      "the cosine similarity between üá≥üá¨ and üì≤ could not be computed.\n",
      "the cosine similarity between üëá and üá≥üá¨ could not be computed.\n",
      "the cosine similarity between üéß and üá≥üá¨ could not be computed.\n",
      "the cosine similarity between üá≥üá¨ and üé∂ could not be computed.\n",
      "the cosine similarity between üëè and ‚Ü™ could not be computed.\n",
      "Der Spearman Rank Correlation Coefficient is SpearmanrResult(correlation=-0.01886830858449247, pvalue=0.6744541653251483)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# extracts the 508 Annotator Results as the Gold-Standard\n",
    "corpus_filename = '../data/EmoSim508.json'\n",
    "corpus = open(corpus_filename).read()\n",
    "annotator_similarity_score_508 = list(array(re.findall('(?<=_Annotator_Agreement\": )(.*?)(?=\\})', corpus)))\n",
    "\n",
    "# glyph_pairs_1016\n",
    "unicode_pairs_1016 = re.findall('(?<=unicodelong\": \"\\\\\\)(.*?)(?=\")', corpus)    \n",
    "glyph_pairs_1016 = [codecs.decode(unicode_pairs_1016[x].replace(str('\\\\\\\\'),str('\\\\')).replace('_',''), 'unicode_escape') for x in range(len(unicode_pairs_1016))]\n",
    "\n",
    "# computation of Cosine Similarity\n",
    "goldstandard = []\n",
    "selftrained = []\n",
    "for x in range(len(annotator_similarity_score_508)):\n",
    "    cosineSimilarity = None\n",
    "    \n",
    "    emoji1 = glyph_pairs_1016.pop(0)\n",
    "    emoji2 = glyph_pairs_1016.pop(0)\n",
    "    \n",
    "    try:\n",
    "        cosineSimilarity = emoji2vec.wv.similarity(emoji1, emoji2)\n",
    "    except:\n",
    "        print('the cosine similarity between ' + emoji1 + ' and ' + emoji2 + ' could not be computed.')\n",
    "    \n",
    "    if(cosineSimilarity is not None):\n",
    "        selftrained.append(cosineSimilarity)\n",
    "        goldstandard.append(annotator_similarity_score_508.pop(0))\n",
    "\n",
    "# computation of SPEARRANK CORRELATION COEFFICIENT\n",
    "spearmanRank = stats.spearmanr(goldstandard, selftrained)\n",
    "\n",
    "print('Der Spearman Rank Correlation Coefficient is {}'.format(spearmanRank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('üòë', 0.5965372323989868),\n",
       " ('üòß', 0.5478870868682861),\n",
       " ('üò¶', 0.5450003147125244),\n",
       " ('üòñ', 0.5350342988967896),\n",
       " ('üò∑', 0.5086485743522644),\n",
       " ('üòµ', 0.4946061968803406),\n",
       " ('üòï', 0.4879322052001953),\n",
       " ('ü§í', 0.4814000427722931),\n",
       " ('üò£', 0.46933263540267944),\n",
       " ('üôÅ', 0.46830254793167114)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji2vec.wv.most_similar('üòü')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
