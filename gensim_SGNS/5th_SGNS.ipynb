{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from numpy  import array\n",
    "from scipy import stats\n",
    "from gensim.models.callbacks import CallbackAny2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_filename = '../data/extracted_emoji_sequences.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(corpus_filename).read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlyEmojiSequences(tokens):\n",
    "    threshold_emojis = [x for x in tokens if len(x) > 1]\n",
    "    return threshold_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "emojiSequences = onlyEmojiSequences(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus contains 610,256 emoji points.\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in emojiSequences])\n",
    "print(\"The corpus contains {0:,} emoji points.\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETTING VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality of the resulting word vectors.\n",
    "num_features = 300\n",
    "\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 2\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "# remove later\n",
    "seed = 1\n",
    "\n",
    "emoji2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{}  Loss-Value: {}\".format(self.epoch, emoji2vec.get_latest_training_loss()))\n",
    "        self.epoch += 1\n",
    "        \n",
    "epoch_logger = EpochLogger()\n",
    "\n",
    "emoji2vec.build_vocab(emojiSequences, progress_per=10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0  Loss-Value: 105278.6640625\n",
      "Epoch #1  Loss-Value: 169150.765625\n",
      "Epoch #2  Loss-Value: 236787.046875\n",
      "Epoch #3  Loss-Value: 291833.6875\n",
      "Epoch #4  Loss-Value: 366358.71875\n",
      "Epoch #5  Loss-Value: 431451.09375\n",
      "Epoch #6  Loss-Value: 483302.0\n",
      "Epoch #7  Loss-Value: 535496.0625\n",
      "Epoch #8  Loss-Value: 586110.25\n",
      "Epoch #9  Loss-Value: 638934.3125\n",
      "Epoch #10  Loss-Value: 698479.8125\n",
      "Epoch #11  Loss-Value: 747007.3125\n",
      "Epoch #12  Loss-Value: 793715.875\n",
      "Epoch #13  Loss-Value: 854693.4375\n",
      "Epoch #14  Loss-Value: 897203.0625\n",
      "Epoch #15  Loss-Value: 954439.5625\n",
      "Epoch #16  Loss-Value: 1004379.875\n",
      "Epoch #17  Loss-Value: 1063259.875\n",
      "Epoch #18  Loss-Value: 1114722.75\n",
      "Epoch #19  Loss-Value: 1180580.375\n",
      "Epoch #20  Loss-Value: 1236854.625\n",
      "Epoch #21  Loss-Value: 1294300.875\n",
      "Epoch #22  Loss-Value: 1338833.875\n",
      "Epoch #23  Loss-Value: 1387425.0\n",
      "Epoch #24  Loss-Value: 1442999.875\n",
      "Epoch #25  Loss-Value: 1496312.625\n",
      "Epoch #26  Loss-Value: 1545001.25\n",
      "Epoch #27  Loss-Value: 1599247.5\n",
      "Epoch #28  Loss-Value: 1664913.75\n",
      "Epoch #29  Loss-Value: 1721520.125\n",
      "Epoch #30  Loss-Value: 1775543.375\n",
      "Epoch #31  Loss-Value: 1822771.875\n",
      "Epoch #32  Loss-Value: 1878923.875\n",
      "Epoch #33  Loss-Value: 1934665.125\n",
      "Epoch #34  Loss-Value: 1991760.125\n",
      "Epoch #35  Loss-Value: 2047948.625\n",
      "Epoch #36  Loss-Value: 2095967.5\n",
      "Epoch #37  Loss-Value: 2140805.5\n",
      "Epoch #38  Loss-Value: 2197947.0\n",
      "Epoch #39  Loss-Value: 2252580.75\n",
      "Epoch #40  Loss-Value: 2307965.0\n",
      "Epoch #41  Loss-Value: 2361193.75\n",
      "Epoch #42  Loss-Value: 2417077.0\n",
      "Epoch #43  Loss-Value: 2471034.25\n",
      "Epoch #44  Loss-Value: 2519295.75\n",
      "Epoch #45  Loss-Value: 2571568.5\n",
      "Epoch #46  Loss-Value: 2625951.75\n",
      "Epoch #47  Loss-Value: 2671950.0\n",
      "Epoch #48  Loss-Value: 2724083.0\n",
      "Epoch #49  Loss-Value: 2786123.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19445935, 30512800)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji2vec.train(emojiSequences, total_examples=emoji2vec.corpus_count, epochs = 50, compute_loss=True, callbacks=[epoch_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji2vec.save(os.path.join(\"trained\", \"2nd.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATING TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"2nd.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cosine similarity between üá¨üáß and üá∫üá∏ could not be computed.\n",
      "the cosine similarity between üèÖ and üá∫üá∏ could not be computed.\n",
      "the cosine similarity between üá∫üá∏ and ‚ù§ could not be computed.\n",
      "the cosine similarity between üá∫üá∏ and üí• could not be computed.\n",
      "the cosine similarity between üé§ and üá≥üá¨ could not be computed.\n",
      "the cosine similarity between üá≥üá¨ and üì≤ could not be computed.\n",
      "the cosine similarity between üëá and üá≥üá¨ could not be computed.\n",
      "the cosine similarity between üéß and üá≥üá¨ could not be computed.\n",
      "the cosine similarity between üá≥üá¨ and üé∂ could not be computed.\n",
      "the cosine similarity between üëè and ‚Ü™ could not be computed.\n",
      "Der Spearman Rank Correlation Coefficient is SpearmanrResult(correlation=0.5226083857910726, pvalue=2.9945937436152867e-36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smoky/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# extracts the 508 Annotator Results as the Gold-Standard\n",
    "corpus_filename = '../data/EmoSim508.json'\n",
    "corpus = open(corpus_filename).read()\n",
    "annotator_similarity_score_508 = list(array(re.findall('(?<=_Annotator_Agreement\": )(.*?)(?=\\})', corpus)))\n",
    "\n",
    "# glyph_pairs_1016\n",
    "unicode_pairs_1016 = re.findall('(?<=unicodelong\": \"\\\\\\)(.*?)(?=\")', corpus)    \n",
    "glyph_pairs_1016 = [codecs.decode(unicode_pairs_1016[x].replace(str('\\\\\\\\'),str('\\\\')).replace('_',''), 'unicode_escape') for x in range(len(unicode_pairs_1016))]\n",
    "\n",
    "# computation of Cosine Similarity\n",
    "goldstandard = []\n",
    "selftrained = []\n",
    "for x in range(len(annotator_similarity_score_508)):\n",
    "    cosineSimilarity = None\n",
    "    \n",
    "    emoji1 = glyph_pairs_1016.pop(0)\n",
    "    emoji2 = glyph_pairs_1016.pop(0)\n",
    "    \n",
    "    try:\n",
    "        cosineSimilarity = emoji2vec.wv.similarity(emoji1, emoji2)\n",
    "    except:\n",
    "        print('the cosine similarity between ' + emoji1 + ' and ' + emoji2 + ' could not be computed.')\n",
    "    \n",
    "    if(cosineSimilarity is not None):\n",
    "        selftrained.append(cosineSimilarity)\n",
    "        goldstandard.append(annotator_similarity_score_508.pop(0))\n",
    "\n",
    "# computation of SPEARRANK CORRELATION COEFFICIENT\n",
    "spearmanRank = stats.spearmanr(goldstandard, selftrained)\n",
    "\n",
    "print('Der Spearman Rank Correlation Coefficient is {}'.format(spearmanRank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('üòß', 0.5780831575393677),\n",
       " ('üòë', 0.5687667727470398),\n",
       " ('üòñ', 0.5309852361679077),\n",
       " ('üò¶', 0.5202800035476685),\n",
       " ('üòØ', 0.4996958374977112),\n",
       " ('ü§í', 0.47872912883758545),\n",
       " ('üò£', 0.47099769115448),\n",
       " ('üòï', 0.470908522605896),\n",
       " ('üò®', 0.46285319328308105),\n",
       " ('üôÅ', 0.4577151834964752)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji2vec.wv.most_similar('üòü')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5035581218413427"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji2vec.wv.similarity('üôÅ', 'ü§í')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important check how results changes with different setting!!\n",
    "# combination of trying out stuff and googling!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
