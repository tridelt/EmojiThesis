{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from numpy  import array\n",
    "from scipy import stats\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_filename = '../data/extracted_emoji_sequences.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(corpus_filename).read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlyEmojiSequences(tokens):\n",
    "    threshold_emojis = [x for x in tokens if len(x) > 1]\n",
    "    return threshold_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "emojiSequences = onlyEmojiSequences(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus contains 610,256 emoji points.\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in emojiSequences])\n",
    "print(\"The corpus contains {0:,} emoji points.\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETTING VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality of the resulting word vectors.\n",
    "num_features = 8\n",
    "\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "# num_workers = 1\n",
    "\n",
    "\n",
    "# Context window length.\n",
    "context_size = 4\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "# remove later\n",
    "seed = 1\n",
    "\n",
    "emoji2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.tempLoss = 0\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{}  Loss-Value: {}\".format(self.epoch, (emoji2vec.get_latest_training_loss() - self.tempLoss)))\n",
    "        self.epoch += 1\n",
    "        self.tempLoss = emoji2vec.get_latest_training_loss()\n",
    "        \n",
    "epoch_logger = EpochLogger()\n",
    "emoji2vec.build_vocab(emojiSequences, progress_per=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0  Loss-Value: 121729.140625\n",
      "Epoch #1  Loss-Value: 84108.328125\n",
      "Epoch #2  Loss-Value: 64453.375\n",
      "Epoch #3  Loss-Value: 81151.09375\n",
      "Epoch #4  Loss-Value: 92107.21875\n",
      "Epoch #5  Loss-Value: 81070.90625\n",
      "Epoch #6  Loss-Value: 94083.5625\n",
      "Epoch #7  Loss-Value: 80113.75\n",
      "Epoch #8  Loss-Value: 77975.0625\n",
      "Epoch #9  Loss-Value: 79303.125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3888538, 6102560)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji2vec.train(emojiSequences, total_examples=emoji2vec.corpus_count, epochs = 10, compute_loss=True, callbacks=[epoch_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")\n",
    "emoji2vec.save(os.path.join(\"trained\", \"canIseed.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATING TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"canIseed.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cosine similarity between 🇬🇧 and 🇺🇸 could not be computed.\n",
      "the cosine similarity between 🏅 and 🇺🇸 could not be computed.\n",
      "the cosine similarity between 🇺🇸 and ❤ could not be computed.\n",
      "the cosine similarity between 🇺🇸 and 💥 could not be computed.\n",
      "the cosine similarity between 🎤 and 🇳🇬 could not be computed.\n",
      "the cosine similarity between 🇳🇬 and 📲 could not be computed.\n",
      "the cosine similarity between 👇 and 🇳🇬 could not be computed.\n",
      "the cosine similarity between 🎧 and 🇳🇬 could not be computed.\n",
      "the cosine similarity between 🇳🇬 and 🎶 could not be computed.\n",
      "the cosine similarity between 👏 and ↪ could not be computed.\n",
      "\n",
      "mein Spearman: 0.6299432186334714\n",
      "sein Spearman: 0.7609726910462977\n",
      "mein MAE ist 0.388885388683199\n",
      "sein MAE ist 0.23891566265060243\n",
      "mein MSE ist 0.20146107536823743\n",
      "sein MSE ist 0.07887625502008032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype <U4 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# extracts the 508 Annotator Results as the Gold-Standard\n",
    "corpus_filename = '../data/EmoSim508.json'\n",
    "corpus = open(corpus_filename).read()\n",
    "annotator_similarity_score_508 = list(array(re.findall('(?<=_Annotator_Agreement\": )(.*?)(?=\\})', corpus)))\n",
    "\n",
    "# extract Wijeratne's Cosine_Similarities of the model which was trained on Google_Sense_Labels\n",
    "google_sense_labels_score_508 = list(array(re.findall('(?<=Google_Sense_Label\": )(.*?)(?=\\,)', corpus)))\n",
    "\n",
    "# glyph_pairs_1016\n",
    "unicode_pairs_1016 = re.findall('(?<=unicodelong\": \"\\\\\\)(.*?)(?=\")', corpus)    \n",
    "glyph_pairs_1016 = [codecs.decode(unicode_pairs_1016[x].replace(str('\\\\\\\\'),str('\\\\')).replace('_',''), 'unicode_escape') for x in range(len(unicode_pairs_1016))]\n",
    "\n",
    "# computation of Cosine Similarity\n",
    "goldstandard = []\n",
    "selftrained = []\n",
    "google_sense_labels = []\n",
    "for x in range(len(annotator_similarity_score_508)):\n",
    "    cosineSimilarity = None\n",
    "    \n",
    "    emoji1 = glyph_pairs_1016.pop(0)\n",
    "    emoji2 = glyph_pairs_1016.pop(0)\n",
    "    \n",
    "    try:\n",
    "        cosineSimilarity = emoji2vec.wv.similarity(emoji1, emoji2)\n",
    "    except:\n",
    "        print('the cosine similarity between ' + emoji1 + ' and ' + emoji2 + ' could not be computed.')\n",
    "    \n",
    "    if(cosineSimilarity is not None):\n",
    "        goldstandard.append(annotator_similarity_score_508.pop(0))\n",
    "        selftrained.append(cosineSimilarity)\n",
    "        google_sense_labels.append(float(google_sense_labels_score_508.pop(0)))\n",
    "        \n",
    "\n",
    "# skalierter GoldStandard\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "scaled_goldstandard = min_max_scaler.fit_transform(np.asarray(goldstandard).reshape(-1, 1))\n",
    "\n",
    "print()\n",
    "\n",
    "# computation of SPEARRANK CORRELATION COEFFICIENT\n",
    "meinSPEARMAN = stats.spearmanr(goldstandard, selftrained)\n",
    "seinSPEARMAN = stats.spearmanr(goldstandard, google_sense_labels)\n",
    "print('mein Spearman: {}'.format(meinSPEARMAN.correlation))\n",
    "print('sein Spearman: {}'.format(seinSPEARMAN.correlation))\n",
    "\n",
    "\n",
    "# computation of MAE\n",
    "meinMAE = mean_absolute_error(scaled_goldstandard, selftrained)\n",
    "seinMAE = mean_absolute_error(scaled_goldstandard, google_sense_labels)\n",
    "print('mein MAE ist {}'.format(meinMAE))\n",
    "print('sein MAE ist {}'.format(seinMAE))\n",
    "\n",
    "\n",
    "# computation of MSE\n",
    "meinMSE = mean_squared_error(scaled_goldstandard, selftrained)\n",
    "seinMSE = mean_squared_error(scaled_goldstandard, google_sense_labels)\n",
    "print('mein MSE ist {}'.format(meinMSE))\n",
    "print('sein MSE ist {}'.format(seinMSE))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
