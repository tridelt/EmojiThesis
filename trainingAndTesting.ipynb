{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as A\n",
    "torch.manual_seed(7)\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn.manifold \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_filename = './data/extracted_emoji_sequences.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(corpus_filename).read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlyEmojiSequences(tokens):\n",
    "    threshold_emojis = [x for x in tokens if len(x) > 1]\n",
    "    return threshold_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "emojiSequences = onlyEmojiSequences(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "# this is just the very basic translation both ways plus the length of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "\n",
    "# sole purpose of this is to have pairs! of target and context word\n",
    "# super simple once you have figured out the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "# this is a one hot encoded something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7)\n",
    "torch.cuda.manual_seed_all\n",
    "\n",
    "dtype = torch.float \n",
    "dimensionSize = 300\n",
    "num_epochs = 0\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "firstLayer = torch.randn(dimensionSize, vocabulary_size, requires_grad=True).float()\n",
    "secondLayer = torch.randn(vocabulary_size, dimensionSize, requires_grad=True).float()\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "\n",
    "\n",
    "        # one hot encoded tensor\n",
    "        x = get_input_layer(data).float()\n",
    "        # target word \n",
    "        y_true = torch.from_numpy(np.array([target])).long()\n",
    "\n",
    "        # Hidden Layer: gradient magic happening ...\n",
    "        z1 = torch.matmul(firstLayer, x)\n",
    "        z2 = torch.matmul(secondLayer, z1)\n",
    "\n",
    "        # introducing non-linearity\n",
    "        softmax = A.LogSoftmax(dim=0)\n",
    "        soft_max_output = softmax(z2)\n",
    "\n",
    "        # neg_log_likelihood\n",
    "        loss = F.nll_loss(soft_max_output.view(1,-1), y_true)\n",
    "        loss_val += loss  # this might be to please the  framework, and adding stuff to the gradient calculator\n",
    "        \n",
    "        # propagating it back\n",
    "        loss.backward()\n",
    "        \n",
    "        # updating the weights of both layers\n",
    "        firstLayer.data -= learning_rate * firstLayer.grad.data\n",
    "        secondLayer.data -= learning_rate * secondLayer.grad.data\n",
    "\n",
    "        # set the gradients to zero for next iteration\n",
    "        firstLayer.grad.data.zero_()\n",
    "        secondLayer.grad.data.zero_()\n",
    "        \n",
    "    # this keeps track of the loss, hopefully it does converge\n",
    "    if epo % 1000 == 0:    \n",
    "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Creates and TSNE model and plots it\"\n",
    "tokens = []\n",
    "\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "\n",
    "new_values = tsne_model.fit_transform(secondLayer.detach().numpy())\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for value in new_values:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "\n",
    "plt.figure(figsize=(16, 16)) \n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    plt.annotate(vocabulary[i],\n",
    "                 xy=(x[i], y[i]),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from numpy  import array\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "\n",
    "for x in range(len(word2idx)):\n",
    "    word2idx[('U+{:X}'.format(ord(idx2word[x])))] = word2idx.pop(idx2word[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_filename = './data/EmoSim508.json'\n",
    "corpus = open(corpus_filename).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myEmojiPairScores = []\n",
    "approved_annotator_results = []\n",
    "all_annotator_results = list(array(re.findall('(?<=_Annotator_Agreement\": )(.*?)(?=\\})', corpus)))\n",
    "\n",
    "# get all unicodes from all 508 pairs\n",
    "unicodeEmojiPairs = re.findall('(?<=unicodeshort\": \")(.*?)(?=\\\")', corpus)\n",
    "\n",
    "for x in range(508):\n",
    "    emoji1 = unicodeEmojiPairs.pop()\n",
    "    emoji2 = unicodeEmojiPairs.pop()\n",
    "    respective_annotator_result = all_annotator_results.pop(0)\n",
    "    if (emoji1 and emoji2 in word2idx) and ((len(emoji1) < 9 ) and (len(emoji2) < 9)):\n",
    "        try:\n",
    "            myEmojiPairScores.append(distance.euclidean(secondLayer.detach().numpy()[word2idx[emoji1]], secondLayer.detach().numpy()[word2idx[emoji2]]))\n",
    "            approved_annotator_results.append(respective_annotator_result)\n",
    "        except:\n",
    "            print(\"didn't work\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.spearmanr(myEmojiPairScores, approved_annotator_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
